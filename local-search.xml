<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/</url>
    
    <content type="html"><![CDATA[<p>本博客此次主要介绍普通爬虫如何爬取，并且扩展如何写scrapy框架和分布式爬虫</p><p>普通爬虫是什么？不知道，就当做基本的爬虫吧</p><p>分布式爬虫主要使用scrapy_redis组件，使用redis提供共享的调度器和管道</p><h4 id="一步步学习分布式爬取b站某up主空间下的视频无法做到全空间视频爬取仍在学习">一步步学习分布式爬取B站某Up主空间下的视频（无法做到全空间视频爬取，仍在学习）</h4><p>本博客不会讲一些基础知识，只讲爬虫怎么使用和学习，不会的知识点需要自己补充</p><blockquote><p>python+基本的网页爬取</p><p>scrapy的编写,虽然也会讲，但是至少也会点，不然还是不懂</p><p>python库的安装（pip install 包 -i https://pypi.douban.com/simple</p><p>html,css,js</p><p>还有个好像是网页数据请求方面的，本人也不会，好像就硬记住了，后面看看是哪方面内容</p><p>redis的基本使用</p></blockquote><h5 id="最简单的网页爬取">最简单的网页爬取</h5><p>我们爬取<code>我是不白吃</code>视频主页的网页源代码（好像是室友自己要爬取这个up主，当时在测试代码）</p><p>url</p><blockquote><p>https://space.bilibili.com/451296298/video</p></blockquote><p>首先我们右键-&gt;查看网页源代码</p><p>或者按F12-&gt;刷新页面-&gt;Source-&gt;video就能看到源代码</p><p>这样我们下面的代码获取的response就是网页源代码，如果我们所需要的信息就是这个，那么后面就是从里面获取数据了</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20221128151311519.png" alt="image-20221128151311519"><figcaption aria-hidden="true">image-20221128151311519</figcaption></figure><p><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20221128152350037.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br>headers = &#123;<br>    <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>&#125;<br>url = <span class="hljs-string">&#x27;https://space.bilibili.com/451296298/video&#x27;</span><br><br><span class="hljs-comment"># 一般使用requests.get()就能获取数据，post很久没遇见了</span><br>response = requests.get(url=url,headers=headers)<br><br><span class="hljs-comment"># 持久化response</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;page.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(response.text)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;url&#125;:网页源代码爬取成功！&quot;</span>.<span class="hljs-built_in">format</span>(url=url))<br></code></pre></td></tr></table></figure><p>网页源代码中没有我们需要需要的数据，所以需要使用其他方法来获取数据</p><h5 id="爬取某up主下30条视频的信息">爬取某Up主下30条视频的信息</h5><p>由于上一个方法没能获取到视频主页的信息，我们使用下面的方法来获取</p><p>F12-&gt;刷新页面-&gt;Network-&gt;Ctrl+F-&gt;输入页面上看见的关键词-&gt;回车-&gt;点击出来选项-&gt;此时能在response中看到数据（这就是最中我们get到的数据）-&gt;点击Header-&gt;可以观察到RequestMethod为Get-&gt;我们需要的url就是RequestURL，改变其中的参数即可-&gt;最后有个参数列表，有的时候会用上，可以看一下</p><p>经过测试主要就是pn和mid起作用，前者是第几页，（第0页和第1页效果相同），mid是uid的值，即你要爬取那个up主，ps好像是每次取多少个，估计也能改变，ps最多取50个，不过有pn起作用，不知道会不会重复</p><p>最后结果返回一个结果，里面包含uid，BV，aid，description，title，使用BV和aid就能爬取一个完整视频的信息了</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20221128153214083.png" alt="image-20221128153214083"><figcaption aria-hidden="true">image-20221128153214083</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20221128153304576.png" alt="image-20221128153304576"><figcaption aria-hidden="true">image-20221128153304576</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B(%E4%B8%80)/Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20221128153512207.png" alt="image-20221128153512207"><figcaption aria-hidden="true">image-20221128153512207</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-keyword">import</span> os<br><br>headers = &#123;<br>    <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>&#125;<br><br><span class="hljs-comment"># uid:里面是uid</span><br><span class="hljs-comment"># max_page:表示爬取Up主主页下的多少页视频信息（每页代表有30个视频，如果数量不足会报错）</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getAidAndBV</span>(<span class="hljs-params">uid,max_page=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-comment"># 里面是两个字典，每个字典分别对应着aid和BV，事实上还有uid,title,description</span><br>    <span class="hljs-comment"># [&#123;&#x27;aid&#x27;:&#x27;&#x27;,&#x27;BV&#x27;:&#x27;&#x27;&#125;,[],]</span><br>    <span class="hljs-comment">#存储爬取后的aid和bv号</span><br>    aidAndBV_list = []<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,max_page+<span class="hljs-number">1</span>):<br>            <span class="hljs-comment"># B站提供的api，能够获得30个视频的信息，是json数据</span><br>            url = <span class="hljs-string">&#x27;https://api.bilibili.com/x/space/arc/search?mid=&#123;uid&#125;&amp;ps=30&amp;tid=0&amp;pn=&#123;page&#125;&amp;order=pubdate&amp;order_avoided=true&amp;jsonp=jsonp&#x27;</span>.<span class="hljs-built_in">format</span>(uid=uid,page=page)<br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-comment"># 获取相应，是json数据</span><br>                response = requests.get(url=url,headers=headers)<br>                <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">30</span>):<br>                    <span class="hljs-comment"># 将数据存储在本地</span><br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./uid—&#123;uid&#125;_Up主主页视频信息&#x27;</span>.<span class="hljs-built_in">format</span>(uid=uid)):<br>                        os.mkdir(<span class="hljs-string">&#x27;./uid—&#123;uid&#125;_Up主主页视频信息&#x27;</span>.<span class="hljs-built_in">format</span>(uid=uid))<br>                    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./uid—&#123;uid&#125;_Up主主页视频信息/uid-&#123;uid&#125;_page-&#123;page&#125;_index-&#123;index&#125;.json&#x27;</span>.<span class="hljs-built_in">format</span>(uid=uid,page=page,index=index+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>                        <span class="hljs-comment"># 包含中文，不能使用ascii编码</span><br>                        <span class="hljs-comment"># 下面一行代码能把response存储在fp中，虽然我也不知道为什么这样写</span><br>                        json.dump(obj=response.json(), fp=fp, ensure_ascii=<span class="hljs-literal">False</span>)<br>                        <span class="hljs-comment"># response.json()转成dict数据了，json有方法能让dict和json互转，具体的操作不清楚，以后再说</span><br>                        response_json = response.json()<br>                        title = response_json[<span class="hljs-string">&#x27;data&#x27;</span>][<span class="hljs-string">&#x27;list&#x27;</span>][<span class="hljs-string">&#x27;vlist&#x27;</span>][index][<span class="hljs-string">&#x27;title&#x27;</span>]<br>                        description = response_json[<span class="hljs-string">&#x27;data&#x27;</span>][<span class="hljs-string">&#x27;list&#x27;</span>][<span class="hljs-string">&#x27;vlist&#x27;</span>][index][<span class="hljs-string">&#x27;description&#x27;</span>]<br>                        aid = response_json[<span class="hljs-string">&#x27;data&#x27;</span>][<span class="hljs-string">&#x27;list&#x27;</span>][<span class="hljs-string">&#x27;vlist&#x27;</span>][index][<span class="hljs-string">&#x27;aid&#x27;</span>]<br>                        BV = response_json[<span class="hljs-string">&#x27;data&#x27;</span>][<span class="hljs-string">&#x27;list&#x27;</span>][<span class="hljs-string">&#x27;vlist&#x27;</span>][index][<span class="hljs-string">&#x27;bvid&#x27;</span>]<br>                        <span class="hljs-comment"># print看一下爬取情况，不然不知道有没有跑起来</span><br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;aid:&#123;&#125;,BV:&#123;&#125;,title:&#123;&#125;,description:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(aid,BV,title,description))<br>                        dic = &#123;<br>                            <span class="hljs-string">&#x27;uid&#x27;</span>:uid,<br>                            <span class="hljs-string">&#x27;title&#x27;</span>:title,<br>                            <span class="hljs-string">&#x27;description&#x27;</span>:description,<br>                            <span class="hljs-string">&#x27;aid&#x27;</span>:aid,<br>                            <span class="hljs-string">&#x27;BV&#x27;</span>:BV,<br>                        &#125;<br>                        aidAndBV_list.append(dic)<br>                        <span class="hljs-comment"># 不要过于快速爬取数据，停止0.5s</span><br>                        sleep(<span class="hljs-number">0.5</span>)<br>            <span class="hljs-keyword">except</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;视频数量不止30条&quot;</span>)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;页面数据不足或没有网络&quot;</span>)<br>    <span class="hljs-keyword">return</span> aidAndBV_list<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># url = &#x27;https://api.bilibili.com/x/v2/reply/main?&amp;next=0&amp;oid=%dplat=1&amp;type=1&#x27;</span><br>    <span class="hljs-comment"># 给定uid，爬取某个uid中主页的信息，改成一个个爬取了，要是想要爬取多个遍历就好</span><br>    <span class="hljs-comment"># 字符串和数字都行，建议字符串</span><br>    uid = <span class="hljs-string">&#x27;451296298&#x27;</span><br>    <span class="hljs-comment"># uid = 451296298</span><br>    <span class="hljs-comment"># max_page:表示爬取Up主主页下的多少页视频信息（每页代表有30个视频，如果数量不足会报错）</span><br>    max_page = <span class="hljs-number">1</span><br>    aidAndBV_list = getAidAndBV(uid=uid,max_page=max_page)<br></code></pre></td></tr></table></figure><h5 id="爬取某个特定视频下的信息">爬取某个特定视频下的信息</h5><p>需要BV号和oid（即上面的BV和aid）</p><p>爬取的信息：评论者的评论和uid+名字</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/</url>
    
    <content type="html"><![CDATA[<h4 id="爬虫第二次学习一">爬虫第二次学习（一）</h4><h5 id="开发者工具介绍">开发者工具介绍</h5><h6 id="elements">Elements</h6><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230221214740950.png" alt="image-20230221214740950"><figcaption aria-hidden="true">image-20230221214740950</figcaption></figure><p>下面图片的Computed是布局</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230221215224019.png" alt="image-20230221215224019"><figcaption aria-hidden="true">image-20230221215224019</figcaption></figure><h6 id="network">network</h6><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230514165957693.png" alt="image-20230514165957693"><figcaption aria-hidden="true">image-20230514165957693</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230514172450435.png" alt="image-20230514172450435"><figcaption aria-hidden="true">image-20230514172450435</figcaption></figure><blockquote><p>Header:</p><p>General：</p><p>Request URL:url地址</p><p>Request Method: 发包的协议</p><p>Status Code:请求的状态码</p><p>Remote Address:服务器请求的ip地址:端口号(端口号443表示时http请求)</p><p>Response Headers:接收包，服务器返回的包</p><p>HTTP/1.1 200 OK http的版本号，状态码</p><p>其他自定义和固定样式的数据</p><p>需要注意返回的cookie</p><p>Request Headers:请求头</p><p>GET / HTTP/1.1 协议请求类型和版本号，自己模拟的需要一样</p><p>Host: www.baidu.com 网址信息</p><p>Pragma: no-cache 对于服务器来说不缓存</p><p>User-Agent: 浏览器信息</p><p>Accept: 是我们想请求的文档的类型</p><p>Accept-Encoding: gzip,deflate,br如果发送的数据包含此行，则浏览器会进行压缩</p><p>然后服务器会进行解压</p><p>Cookie:一些cookie信息</p></blockquote><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230514175723135.png" alt="image-20230514175723135"><figcaption aria-hidden="true">image-20230514175723135</figcaption></figure><p>对包进行右键得出的页面</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230514191932529.png" alt="image-20230514191932529"><figcaption aria-hidden="true">image-20230514191932529</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515095246973.png" alt="image-20230515095246973"><figcaption aria-hidden="true">image-20230515095246973</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515103229991.png" alt="image-20230515103229991"><figcaption aria-hidden="true">image-20230515103229991</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515101503691.png" alt="image-20230515101503691"><figcaption aria-hidden="true">image-20230515101503691</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515103221829.png" alt="image-20230515103221829"><figcaption aria-hidden="true">image-20230515103221829</figcaption></figure><h6 id="application">Application</h6><p>clear site data:清除当前网站当前页面的所有缓存</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515211904978.png" alt="image-20230515211904978"><figcaption aria-hidden="true">image-20230515211904978</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515212441001.png" alt="image-20230515212441001"><figcaption aria-hidden="true">image-20230515212441001</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515212846042.png" alt="image-20230515212846042"><figcaption aria-hidden="true">image-20230515212846042</figcaption></figure><p>开启无痕窗口，能够清楚缓存和变量的值。有的时候能够避免掉一些问题</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230515212913801.png" alt="image-20230515212913801"><figcaption aria-hidden="true">image-20230515212913801</figcaption></figure><h5 id="一些基本技能">一些基本技能</h5><h6 id="打开开发者工具">打开开发者工具</h6><ul><li>F12键</li><li><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%80)/assets\image-20230221213805877.png" alt="image-20230221213805877"><figcaption aria-hidden="true">image-20230221213805877</figcaption></figure></li><li>Ctrl + Shift + i（谷歌浏览器）</li></ul><h5 id="快捷键">快捷键</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Ctrl + Shift + i #谷歌浏览器打开开发者工具<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/</url>
    
    <content type="html"><![CDATA[<h4 id="伪造的基本功">伪造的基本功</h4><h5 id="dom-bom和js引擎的关系">DOM BOM和JS引擎的关系</h5><blockquote><p>DOM（Document Object Model）、BOM（Browser ObjectModel）和JS引擎（JavaScript引擎）是构成JavaScript运行环境的三个重要组成部分。</p><ol type="1"><li>DOM：DOM是用于操作和管理网页文档的编程接口。它将网页的结构和内容表示为一个树状结构，每个节点都是一个对象，通过DOM可以对网页的元素进行增删改查操作。DOM提供了一系列的API，可以通过JavaScript代码来访问和操作网页的元素、属性、样式等。DOM操作使得JavaScript可以与网页内容进行交互和动态修改。</li><li>BOM：BOM是JavaScript提供的用于操作浏览器窗口和浏览器功能的编程接口。它提供了一系列的对象和方法，用于获取和控制浏览器窗口、处理用户输入、操作浏览器历史记录、执行跳转等功能。BOM的核心对象是window对象，它代表浏览器窗口，通过window对象可以访问和操作BOM提供的各种功能。</li><li>JS引擎：JS引擎是JavaScript的核心组件，负责解析和执行JavaScript代码。JS引擎将JavaScript代码转换为可执行的指令，执行代码中的逻辑操作和算法。常见的JS引擎有V8（用于Chrome浏览器）、SpiderMonkey（用于Firefox浏览器）等。JS引擎负责解析和执行JavaScript代码，并提供了一些内置对象和函数，如Array、Math、Date等，以及与DOM和BOM进行交互的能力。</li></ol><p>综上所述，DOM和BOM是JavaScript运行环境的组成部分，而JS引擎是负责解析和执行JavaScript代码的核心组件。JavaScript通过JS引擎执行代码，并通过DOM和BOM与网页内容和浏览器窗口进行交互。</p></blockquote><h5 id="伪造环境的用处">伪造环境的用处</h5><blockquote><p>在爬虫技术中，伪造环境是指模拟真实用户访问网站的环境，以使爬虫看起来更像是真实用户的行为。伪造环境的主要用途包括以下几个方面：</p><ol type="1"><li>绕过反爬虫机制：许多网站会采取反爬虫策略，例如限制访问频率、验证用户身份等。通过伪造环境，爬虫可以模拟真实用户的行为，避免被识别为爬虫而被封禁或限制访问。</li><li>获取动态内容：某些网站的内容是通过JavaScript动态加载的，只有在真实浏览器环境下才能获取完整的页面内容。通过伪造环境，爬虫可以模拟浏览器的行为，执行JavaScript代码，从而获取到动态加载的内容。</li><li>爬取登录后的数据：有些网站的数据需要用户登录后才能访问，通过伪造环境，爬虫可以模拟用户的登录过程，获取登录后的数据。</li><li>处理验证码：某些网站为了防止机器人访问，会设置验证码来验证用户的身份。通过伪造环境，爬虫可以模拟用户输入验证码的过程，从而绕过验证码的限制。</li><li>保护隐私和安全：在进行爬取操作时，通过伪造环境可以隐藏爬虫的真实身份和IP地址，保护爬虫的隐私和安全。</li></ol><p>总之，通过伪造环境，爬虫可以更好地模拟真实用户的行为，以获取目标网站的数据，并绕过一些反爬虫机制和限制。但需要注意，合法的爬虫应该遵守网站的规则和政策，并尊重网站的隐私和安全。</p></blockquote><h5 id="我就是浏览器">我就是浏览器</h5><h6 id="dom和bom的含义和作用">DOM和BOM的含义和作用</h6><blockquote><ol type="1"><li>DOM（Document ObjectModel）：DOM是用来表示和操作HTML、XML文档的标准编程接口。它将文档中的每个元素（如标签、属性和文本内容）都作为对象来表示，形成一个树状结构，通过DOM可以对文档的结构和内容进行增删改查操作。DOM提供了一系列的API（应用程序接口）来访问和操作HTML元素，例如通过getElementById()获取特定ID的元素，通过appendChild()在元素中添加子元素等。DOM是JavaScript与网页内容交互的主要手段。</li><li>BOM（Browser ObjectModel）：BOM是JavaScript提供的用于操作浏览器窗口的对象模型。BOM提供了一系列的对象和方法，用于获取和控制浏览器窗口及其相关信息，例如浏览器的窗口大小、地址栏URL、浏览器的历史记录等。BOM的核心对象是window对象，它表示整个浏览器窗口，通过window对象可以访问和操作BOM提供的各种功能。</li></ol><p>总结起来，DOM主要用于操作网页的HTML结构和内容，而BOM则用于操作浏览器窗口及其相关功能。通过DOM和BOM，JavaScript可以与用户界面进行交互、操作网页内容、响应用户操作，并实现与浏览器的交互。</p></blockquote><p>DOM的作用：</p><p>1.对Html进行增删改查</p><p>2.对html的数据格式进行实现</p><p>3.渲染Canvas（画布）</p><p>BOM的作用：</p><ol type="1"><li>浏览器自己实现的一些类 location，navigator</li><li>open ssl实现了btoa,AES,DES,MD5这些传统加密方式</li></ol><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230713112814020.png" alt="image-20230713112814020"><figcaption aria-hidden="true">image-20230713112814020</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230713113034281.png" alt="image-20230713113034281"><figcaption aria-hidden="true">image-20230713113034281</figcaption></figure><p>JS引擎是一种解释器，能解释js代码，但是又es5和es6两种版本的js，运行时有差别，需要自己区别这两种引擎</p><p>谷歌v8 微软查克拉 ffg quickjs</p><p>加密信息：</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230713113913823.png" alt="image-20230713113913823"><figcaption aria-hidden="true">image-20230713113913823</figcaption></figure><p>DOM和BOM都是js对象</p><p>浏览器去实现的对象，会添加只读属性的特性</p><p>我们不想改变源代码js，但支持多性能，使其爬取速度快</p><ul><li>扣取js 删除环境的一些代码</li><li>伪造环境 伪造环境代码<ul><li>简单的网站 扣就能解决</li><li>复杂的网站 伪造环境能解决</li></ul></li><li>如何伪造<ul><li>要伪造什么<ul><li>全部伪造python里面的jsdom，实现了完整的dom环境。nodejs实现了完整的DOM环境和BOM环境（但是很容易被检测到）</li><li>每个网站检测的东西不多，只需要给指定的网站伪造指定的部分就行</li><li>那如何知道网站检测了什么？1.通过调试 2.全局异常捕获3.本地环境运行看报错</li></ul></li></ul></li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230713195909925.png" alt="image-20230713195909925"><figcaption aria-hidden="true">image-20230713195909925</figcaption></figure><p>selenium 可以通过开一个进程来替代渲染，但是开放的权限很低</p><p>可以通过下面的Fiddler来进行伪装测试</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230713203048553.png" alt="image-20230713203048553"><figcaption aria-hidden="true">image-20230713203048553</figcaption></figure><h4 id="反调试">反调试</h4><p>检测是否在调试</p><ul><li>键盘监听，查看是否按了F12</li><li>检测浏览器内外的宽度差值</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714100932037.png" alt="image-20230714100932037"><figcaption aria-hidden="true">image-20230714100932037</figcaption></figure><ul><li>检测开发这人员工具变量是否为True，如果为True则表示打开了开发者工具</li><li>利用console，一般按F12后会打开控制台，就会console进行使用，所以可以利用console.log()来进行测试判断是否在调试</li><li>利用代码的运行时间差，由于调试时会暂停，所以代码时间差会被改变</li><li>利用toString，在查看某个函数或者方法的作用时，会调用toString来显示，可以利用这个来查看是否被调试</li><li>检测栈的层数xx.caller来检测是否被调试，如果不是自己定义的函数，则是被调试中</li><li>检测非浏览器</li></ul><h5 id="调试的分类">调试的分类</h5><ul><li><p>显性</p><ul><li><p>debugger</p><ul><li><p>非虚拟机</p><ul><li>直接编辑断点信息为False</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714105851546.png" alt="image-20230714105851546"><figcaption aria-hidden="true">image-20230714105851546</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714105744986.png" alt="image-20230714105744986"><figcaption aria-hidden="true">image-20230714105744986</figcaption></figure><ul><li><p>直接替换代码</p><ul><li>谷歌内核浏览器默认提供</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714110626496.png" alt="image-20230714110626496"><figcaption aria-hidden="true">image-20230714110626496</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714111023452.png" alt="image-20230714111023452"><figcaption aria-hidden="true">image-20230714111023452</figcaption></figure><ul><li>浏览器的插件 油猴（在github上）</li><li>代理替换（小提琴，Fiddler）：让流量走我们的软件，我们的软件进行一定的判断，再替换其中的文件</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714111550752.png" alt="image-20230714111550752"><figcaption aria-hidden="true">image-20230714111550752</figcaption></figure><p>挂代理，然后刷新</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714111654805.png" alt="image-20230714111654805"><figcaption aria-hidden="true">image-20230714111654805</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714111823527.png" alt="image-20230714111823527"><figcaption aria-hidden="true">image-20230714111823527</figcaption></figure><p>这种操作允许正则</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714111915395.png" alt="image-20230714111915395"><figcaption aria-hidden="true">image-20230714111915395</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714112028874.png" alt="image-20230714112028874"><figcaption aria-hidden="true">image-20230714112028874</figcaption></figure></li></ul></li><li><p>虚拟机（eval Function）瑞数</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714112248352.png" alt="image-20230714112248352"><figcaption aria-hidden="true">image-20230714112248352</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714112337541.png" alt="image-20230714112337541"><figcaption aria-hidden="true">image-20230714112337541</figcaption></figure><p>下面的通过constructor来构造，如果有debugger(具体也不太清楚)，会返回空函数，干掉debugger，否则正常返回</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%94)/back_up\博客文章\爬虫\assets\image-20230714113918341.png" alt="image-20230714113918341"><figcaption aria-hidden="true">image-20230714113918341</figcaption></figure></li></ul></li><li><p>死循环、循环语句、无限递归、两个方法互相调用、计时器、（打开新页面，写你的历史记录，url无限长等）</p></li><li><p>for(;;)</p></li><li><p>while(true)</p></li></ul></li><li><p>隐形（暗桩）</p><ul><li>引向错误的逻辑</li><li>浏览器正常堆栈</li></ul></li></ul><p><font color="red">P17,40：00案例</font></p><h5 id="hook">hook</h5>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/</url>
    
    <content type="html"><![CDATA[<h4 id="代码混淆">代码混淆</h4><blockquote><p>将正常的代码进行语法上的改变，变量名/名称上的改变，混淆大家逆向js的目的</p></blockquote><ul><li><p>eval混淆</p><blockquote><p>eval()：创建新的虚拟机，在新的虚拟机上运行新的js代码，这是赋予js的能力，js原本没有这个方法---&gt;可以运行js代码</p></blockquote></li><li><p>AA和OO混淆</p></li><li><p>JSFUCK</p></li></ul><h5 id="eval混淆">eval混淆</h5><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230520150555801.png" alt="image-20230520150555801"><figcaption aria-hidden="true">image-20230520150555801</figcaption></figure><p>可以进行多次混淆，但是最终的明文可以用括号括住来拿到结果</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230520152553557.png" alt="image-20230520152553557"><figcaption aria-hidden="true">image-20230520152553557</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230520152654753.png" alt="image-20230520152654753"><figcaption aria-hidden="true">image-20230520152654753</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230520152856753.png" alt="image-20230520152856753"><figcaption aria-hidden="true">image-20230520152856753</figcaption></figure><h5 id="aa和oo混淆">AA和OO混淆</h5><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230704192404655.png" alt="image-20230704192404655"><figcaption aria-hidden="true">image-20230704192404655</figcaption></figure><p>js加密网址：&lt;www.sojson.com&gt;</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230704192801523.png" alt="image-20230704192801523"><figcaption aria-hidden="true">image-20230704192801523</figcaption></figure><p>利用eval封装实际要运行的函数</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">Function1</span>(<span class="hljs-params">x</span>)<br>&#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">eval</span>(<span class="hljs-string">&quot;(function()&#123;return &quot;</span> + x + <span class="hljs-string">&quot;&#125;)&quot;</span>)<br>&#125;<br></code></pre></td></tr></table></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230704195232685.png" alt="image-20230704195232685"><figcaption aria-hidden="true">image-20230704195232685</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230704195112685.png" alt="image-20230704195112685"><figcaption aria-hidden="true">image-20230704195112685</figcaption></figure><h5 id="jj加密">JJ加密</h5><p>和oo混淆一样，都是通过eval()和Function来进行混淆</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230705115606369.png" alt="image-20230705115606369"><figcaption aria-hidden="true">image-20230705115606369</figcaption></figure><p>通过重定义eval和Function，来进行debugger</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230705120121561.png" alt="image-20230705120121561"><figcaption aria-hidden="true">image-20230705120121561</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230705120537300.png" alt="image-20230705120537300"><figcaption aria-hidden="true">image-20230705120537300</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230705121139374.png" alt="image-20230705121139374"><figcaption aria-hidden="true">image-20230705121139374</figcaption></figure><p>也能拿到参数值</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230705121303227.png" alt="image-20230705121303227"><figcaption aria-hidden="true">image-20230705121303227</figcaption></figure><h4 id="fuckjs">FuckJS</h4><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230713103403776.png" alt="image-20230713103403776"><figcaption aria-hidden="true">image-20230713103403776</figcaption></figure><p>加密后的结果如下图所示,一般解密可以通过网上弄好的解密方法来进行解密</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230713104031043.png" alt="image-20230713104031043"><figcaption aria-hidden="true">image-20230713104031043</figcaption></figure><p>自己进行调试</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230713105607182.png" alt="image-20230713105607182"><figcaption aria-hidden="true">image-20230713105607182</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%9B%9B)/back_up\博客文章\爬虫\assets\image-20230713105905172.png" alt="image-20230713105905172"><figcaption aria-hidden="true">image-20230713105905172</figcaption></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/</url>
    
    <content type="html"><![CDATA[<h5 id="抓包步骤">抓包步骤</h5><ul><li><p>抓包</p></li><li><p>调试</p><ul><li>password</li><li>userid</li><li>XHR</li></ul></li><li><p>扣取js</p><ul><li><p>扣全了吗</p></li><li><p>this是谁</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">var</span> zhiyuan = &#123;<br>name : <span class="hljs-string">&quot;ZHIYUAN&quot;</span>,<br>    md5 : <span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)<br>    &#123;<br>        <br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li></ul></li><li><p>改写</p></li><li><p>本地运行出值</p></li><li><p>请求服务器拿值</p></li></ul><h5 id="常见的一些加密方式">常见的一些加密方式</h5><blockquote><p>取盐校验 不可逆</p><p>md5 md2 md4 带密码的md5 ( hmac)</p><p>md5</p><p>​ 默认key 0123456789abcdef</p><p>​ 16位的 32位的 40位的</p><p>​ 123456 <strong>49</strong>ba59abbe56e057<strong>e10</strong>adc3949ba59abbe56e057f20f883e</p><p>sha1 sha256 sha512</p><p>​ 40位 64位 128位</p><p>​ 123456 <strong>7c</strong>4a8d09ca3762af6le59520943dc26494f8941b</p><p>对称加密</p><p>​ AES DES 3DES</p><p>非对称加密</p><p>RSA(私钥 公钥，一般会进行base64的加密操作）同一个明文可以生成不同密文</p><p>1.16进制的</p><p>2.bs64 AL5riQzgwmMhXZX+5MtUOA=<strong>=</strong></p><p>A一Z a-z 0-9 + _ = /</p></blockquote><blockquote><p>123456sohu.com 就是加盐，在明文上加了一些我们不知道的数据</p></blockquote><h6 id="安装fidder工具">安装Fidder工具</h6><p><a href="https://www.telerik.com/fiddler" class="uri">https://www.telerik.com/fiddler</a></p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519094155295.png" alt="image-20230519094155295"><figcaption aria-hidden="true">image-20230519094155295</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519105148662.png" alt="image-20230519105148662"><figcaption aria-hidden="true">image-20230519105148662</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519110619291.png" alt="image-20230519110619291"><figcaption aria-hidden="true">image-20230519110619291</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519110836154.png" alt="image-20230519110836154"><figcaption aria-hidden="true">image-20230519110836154</figcaption></figure><h6 id="md5魔法值">MD5魔法值</h6><p>要使用的一些常量，如果没有会报错</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519111026693.png" alt="image-20230519111026693"><figcaption aria-hidden="true">image-20230519111026693</figcaption></figure><p>实际慢慢调试情况</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519190922135.png" alt="image-20230519190922135"><figcaption aria-hidden="true">image-20230519190922135</figcaption></figure><h6 id="只扣md5函数this部分不扣">只扣md5函数，this部分不扣</h6><p>代码运行快捷键是<code>Ctrl+Enter</code>，右下角有</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519192517426.png" alt="image-20230519192517426"><figcaption aria-hidden="true">image-20230519192517426</figcaption></figure><h6 id="全都扣下来">全都扣下来</h6><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519194421505.png" alt="image-20230519194421505"><figcaption aria-hidden="true">image-20230519194421505</figcaption></figure><h6 id="形成了闭包进行下面的处理使其能在window中能看见闭包就是自己引用自己作用域是局部的其他的使用不了后面的方法使用var进行定义">形成了闭包，进行下面的处理，使其能在window中能看见（闭包就是自己引用自己，作用域是局部的，其他的使用不了），后面的方法使用var进行定义</h6><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%89)/back_up\博客文章\爬虫\assets\image-20230519195538325.png" alt="image-20230519195538325"><figcaption aria-hidden="true">image-20230519195538325</figcaption></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/</url>
    
    <content type="html"><![CDATA[<h4 id="websocket">websocket</h4><p>文本格式</p><ul><li>json xml 纯文本</li><li>自定义格式</li><li>字节码（protobuf(游戏)，TLV(通用)）</li></ul><p>套接字</p><ul><li>tcp/udp（粘包，发送包的时候一次性发了两个时间间隔很短的包，导致接受的时候一起接受，需要进行一定的处理，分开两个包）</li></ul><p>websocket 不是v8引擎白带的，本身就是个关键词</p><blockquote><p>var socket = new websocket (url, [protocol] ) ; socket.readystate</p><p>socket.onopen 连接建立时触发</p><p>socket.onmessage 客户端接收服务端数据时触发</p><p>socket.send() 使用连接发送数据</p><ol type="1"><li>搜索下断，但是会搜到很多</li><li>找到Socket对象，hook，send方法，直接快速</li></ol><p>假设p是Socket对象</p><p>//TODO hook</p><p>p.send_ = p.send;</p><p>p.send = function(x){</p><p>​ debugger;</p><p>​ return p.send_(x);</p><p>}</p></blockquote><p>练习网站 <a href="http://kedou.workerman.net/" class="uri">http://kedou.workerman.net/</a></p><ul><li>抓包</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715114643281.png" alt="image-20230715114643281"><figcaption aria-hidden="true">image-20230715114643281</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715114755154.png" alt="image-20230715114755154"><figcaption aria-hidden="true">image-20230715114755154</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715114933216.png" alt="image-20230715114933216"><figcaption aria-hidden="true">image-20230715114933216</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115105204.png" alt="image-20230715115105204"><figcaption aria-hidden="true">image-20230715115105204</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115208239.png" alt="image-20230715115208239"><figcaption aria-hidden="true">image-20230715115208239</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115343855.png" alt="image-20230715115343855"><figcaption aria-hidden="true">image-20230715115343855</figcaption></figure><p>找到onopen、onmessage这些方法在哪里</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115341515.png" alt="image-20230715115341515"><figcaption aria-hidden="true">image-20230715115341515</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115650867.png" alt="image-20230715115650867"><figcaption aria-hidden="true">image-20230715115650867</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115727026.png" alt="image-20230715115727026"><figcaption aria-hidden="true">image-20230715115727026</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715115818616.png" alt="image-20230715115818616"><figcaption aria-hidden="true">image-20230715115818616</figcaption></figure><p>把下面的代码输入到控制台上，每次调试遇到WebSocket产生的p都需要进行hook，最后调试会产生断点，继续运行，在断住的地方查看上一个断点，如果是send()则可以断住，就能够找到关键的send()了</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">//假设p是Socket对象</span><br><br><span class="hljs-comment">//TODO hook</span><br><br>p.<span class="hljs-property">send_</span> = p.<span class="hljs-property">send</span>;<br><br>p.<span class="hljs-property">send</span> = <span class="hljs-keyword">function</span>(<span class="hljs-params">x</span>)&#123;<br><br><span class="hljs-keyword">debugger</span>;<br><br><span class="hljs-keyword">return</span> p.<span class="hljs-title function_">send_</span>(x);<br><br>&#125;<br></code></pre></td></tr></table></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715193307378.png" alt="image-20230715193307378"><figcaption aria-hidden="true">image-20230715193307378</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715194124446.png" alt="image-20230715194124446"><figcaption aria-hidden="true">image-20230715194124446</figcaption></figure><p>找到r包含了关键信息，和d有关，所以继续分析d</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715194435009.png" alt="image-20230715194435009"><figcaption aria-hidden="true">image-20230715194435009</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715194704766.png" alt="image-20230715194704766"><figcaption aria-hidden="true">image-20230715194704766</figcaption></figure><p>可以看到上面图片最初的引用是wIU9，找到被引用的地方，然后就能定位</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715195356637.png" alt="image-20230715195356637"><figcaption aria-hidden="true">image-20230715195356637</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230715195755683.png" alt="image-20230715195755683"><figcaption aria-hidden="true">image-20230715195755683</figcaption></figure><p>最后就是根据需要的内容，来获取弹幕发送的信息（要有userID，Wup等，都需要扣取下来。）</p><ul><li>分析包的信息</li><li>调试</li><li>本地运行</li></ul><h4 id="练习案例1">练习案例1</h4><p>找cookie值，四个都是cookie，但是我们需要找v相关的那个</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230716103248308.png" alt="image-20230716103248308"><figcaption aria-hidden="true">image-20230716103248308</figcaption></figure><p>有时候变量显示的是空，但可能是unicode显示不出来，实际上不是空，需要再次判断一下。</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230716105031804.png" alt="image-20230716105031804"><figcaption aria-hidden="true">image-20230716105031804</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230716111327432.png" alt="image-20230716111327432"><figcaption aria-hidden="true">image-20230716111327432</figcaption></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-variable language_">console</span>.<span class="hljs-title function_">clear</span>() <span class="hljs-comment">//清空控制台</span><br></code></pre></td></tr></table></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230716112159699.png" alt="image-20230716112159699"><figcaption aria-hidden="true">image-20230716112159699</figcaption></figure><p>根据指定的断点链接来获取包数据</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%B8%83)/back_up\博客文章\爬虫\assets\image-20230807211727775.png" alt="image-20230807211727775"><figcaption aria-hidden="true">image-20230807211727775</figcaption></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/</url>
    
    <content type="html"><![CDATA[<h4 id="极验滑块-底图还原">极验滑块 底图还原</h4><ul><li><p>滑块流程 极验（<a href="https://www.geetest.com/" class="uri">https://www.geetest.com/</a>）</p><ul><li><p>https://www.geetest.com/demo能进行测试</p><ul><li><p>首先开启无痕模式，打开F12</p></li><li><p>进入上面的demo网址</p></li><li><p>分析抓取到的包</p><ul><li>1<ul><li>challlenge:xxx</li><li>gt:xxx</li></ul></li><li>2<ul><li>gettype.php提交了gt,获取到了js文件，获取到了滑块js文件，如fullpage.9.0.2.js</li></ul></li><li>3<ul><li>get.php 提交了gt,challenge,w(加密过，可能是环境校验和轨迹)</li></ul></li><li>4<ul><li>ajax.php 提交gt,challenge,w(加密过，可能是环境校验和轨迹)</li></ul></li><li>5<ul><li>get.php 提交gt,challenge返回了原图（乱码的）、遮罩图（乱码的）滑动图</li></ul></li><li>6<ul><li>ajax.php 提交gt,challenge,w(加密过，可能是环境校验和轨迹)</li><li>返回了validate</li></ul></li></ul></li><li><p>调试</p><ul><li><p>目的</p><ul><li><p>1.找到并分析乱码原图和还原的代码</p><p>%26 * 12 + 1特征码</p><p>还原顺序</p><p>[39,28...]</p><p>312 / 26 = 12px 实际上每个小块宽度只取了10像素</p><p>2.找到并分析w参数如何生成</p><p>“077" 特征码</p><p>会找到特征码=s+c，然后找到s和c是如何产生的，计算特征码</p></li><li><p>调试的知识点</p><ul><li><ol type="1"><li>for</li></ol>​switch 平坦流</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">//平坦流</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">XX</span>(<span class="hljs-params">X</span>)&#123;<br>    <span class="hljs-keyword">return</span> X+<span class="hljs-number">2</span>;<br>&#125;<br><br><span class="hljs-keyword">var</span> a = <span class="hljs-number">1</span>;<br>a = <span class="hljs-title function_">XX</span>(a);<br>a = a-<span class="hljs-number">1</span>;<br><span class="hljs-comment">//不改变代码原执行流程，但是改变了原代码的书写流程</span><br></code></pre></td></tr></table></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230714181821898.png" alt="image-20230714181821898"><figcaption aria-hidden="true">image-20230714181821898</figcaption></figure></li></ul><p>下面的事件监听断点里面有Canvas的创建断点，勾选上能够在创建画布的时候停住，不进行画布创建</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230714165457958.png" alt="image-20230714165457958"><figcaption aria-hidden="true">image-20230714165457958</figcaption></figure><ul><li><p>跟值技巧</p><ul><li>从头看<ul><li>优点：不需要重复下断</li><li>缺点：要记住很多的变量值，不太适合新手</li></ul></li><li>从尾看<ul><li>优点：跟值比较轻松</li><li>缺点：可能会有重复下断点的操作</li></ul></li></ul></li><li><p>下断点</p><ul><li>初始值的位置</li><li>循环的位置</li><li>返回的位置</li><li>函数的开头</li><li>函数的结尾</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">var</span> r = <span class="hljs-keyword">new</span> <span class="hljs-title function_">K</span>()[<span class="hljs-title function_">MTKJ</span>(<span class="hljs-number">308</span>)](t[<span class="hljs-title class_">LZsM</span>(<span class="hljs-number">778</span>)](e))<br><span class="hljs-comment">//可以看到这可能是加密</span><br><span class="hljs-comment">//然后需要找明文是如何来的</span><br></code></pre></td></tr></table></figure></li><li><p>解密思路</p><ul><li><p>扣出加密函数</p></li><li><p>大概率是rsa，找密钥</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-comment">//步骤</span><br><span class="hljs-comment">//1. new rsa</span><br><span class="hljs-comment">//2. setpubkey 设置公钥</span><br><span class="hljs-comment">//3. encrypt 加密</span><br></code></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul></li><li><p>滑块是什么？</p><ul><li>区分是否为机器</li><li>有一个滑动条</li></ul><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715095246640.png" alt="image-20230715095246640"><figcaption aria-hidden="true">image-20230715095246640</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715095302439.png" alt="image-20230715095302439"><figcaption aria-hidden="true">image-20230715095302439</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715101237801.png" alt="image-20230715101237801"><figcaption aria-hidden="true">image-20230715101237801</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715101747173.png" alt="image-20230715101747173"><figcaption aria-hidden="true">image-20230715101747173</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715103228110.png" alt="image-20230715103228110"><figcaption aria-hidden="true">image-20230715103228110</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715103912195.png" alt="image-20230715103912195"><figcaption aria-hidden="true">image-20230715103912195</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715105020655.png" alt="image-20230715105020655"><figcaption aria-hidden="true">image-20230715105020655</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715105655909.png" alt="image-20230715105655909"><figcaption aria-hidden="true">image-20230715105655909</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715105724676.png" alt="image-20230715105724676"><figcaption aria-hidden="true">image-20230715105724676</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AD)/assets\image-20230715110455756.png" alt="image-20230715110455756"><figcaption aria-hidden="true">image-20230715110455756</figcaption></figure></li></ul></li><li><p>看代码分析流程</p></li><li><p>过滑块总结</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">极验的比较简单，想看云盾和易盾的<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/</url>
    
    <content type="html"><![CDATA[<h5 id="明确js逆向的目的">明确js逆向的目的</h5><p>通过本地运行js的文件或者代码，以实现脱离网站和浏览器，并且拿到和浏览器一样的加密的结果</p><blockquote><p>目标：拿到js整个的加密过程，并且在本地的js引擎或js环境中执行起来，拿到加密/解密后的结果。这样就不需要浏览器，直接对服务器发送信息，然后拿到相应的结果。</p></blockquote><h5 id="section"></h5><ul><li>能对网站的js进行调试<ul><li>能修改js运行当中的一些变量的值，能输出</li><li>能下断点</li><li>更智能的监听一些值</li></ul></li></ul><blockquote><p>网站代码运行的时间轴</p><p>加载html -&gt; 加载js -&gt; 运行js初始化 -&gt; 用户触发了某个事件-&gt; 调用了某段js -&gt; 明文数据 -&gt; 加密函数 -&gt; 加密后的数据-&gt; 给服务器发信息(XHR包创建 -&gt; 发送数据) -&gt; 接收到服务器数据-&gt; 解密函数 -&gt; 刷新网页渲染</p></blockquote><h5 id="断点详解">断点详解</h5><ul><li><p>什么是断点</p><ul><li><p>DOM断点（浏览器对象的断点，比如渲染网站时到昵称时，下一个断点，不然它继续渲染）</p><ul><li>定位的比较准</li><li>执行的比较靠前，距离加密函数比较远，我们无法根据栈去快速定位</li></ul></li><li><p>DOM事件断点（点击输入框之后干的事情）</p><ul><li>如果DOM断点不能下断，就可以用DOM事件</li><li>执行的比较靠前，距离加密函数比较远，我们无法根据栈去快速定位</li></ul></li><li><p>XHR断点（阻止浏览器发包）</p><ul><li>执行的比较靠后，距离加密函数相对比较近，可以根据栈快速定位</li><li>非XHR发送的就断不住</li></ul></li><li><p>代码行断点（最基础的断点，手动点击某一行进行断点）</p></li><li><p>代码断点（debugger; ，写在代码行中的代码）</p></li><li><p>全局事件断点（浏览器事件断点）</p></li><li><p>异常捕获断点</p><ul><li>常用于某些浏览器参数</li></ul></li></ul></li></ul><p><strong>DOM断点</strong>，下面的DOM断点，当渲染或者刷新时发生改变才会断点</p><p>先选中要打断点的按钮，然后右键代码部分，break on -&gt; attributemodifications</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516191036376.png" alt="image-20230516191036376"><figcaption aria-hidden="true">image-20230516191036376</figcaption></figure><p>可以看到发生了改变，被断点定住了</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516191218965.png" alt="image-20230516191218965"><figcaption aria-hidden="true">image-20230516191218965</figcaption></figure><p>DOM事件断点</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516195724891.png" alt="image-20230516195724891"><figcaption aria-hidden="true">image-20230516195724891</figcaption></figure><h5 id="全局事件断点">全局事件断点</h5><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516201003901.png" alt="image-20230516201003901"><figcaption aria-hidden="true">image-20230516201003901</figcaption></figure><h5 id="异常捕获断点">异常捕获断点</h5><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516201759085.png" alt="image-20230516201759085"><figcaption aria-hidden="true">image-20230516201759085</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516201810368.png" alt="image-20230516201810368"><figcaption aria-hidden="true">image-20230516201810368</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230516201906693.png" alt="image-20230516201906693"><figcaption aria-hidden="true">image-20230516201906693</figcaption></figure><h5 id="xhr断点">XHR断点</h5><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518154800478.png" alt="image-20230518154800478"><figcaption aria-hidden="true">image-20230518154800478</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518155229613.png" alt="image-20230518155229613"><figcaption aria-hidden="true">image-20230518155229613</figcaption></figure><p>下XHR断点</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518155431721.png" alt="image-20230518155431721"><figcaption aria-hidden="true">image-20230518155431721</figcaption></figure><p>可以看到再次放松验证码的时候自己停住了，这就是使用XHR断点后的效果（换成B站注册，qq注册不发送消息，要求手机发送消息）</p><p>call stack为方法栈</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518155848235.png" alt="image-20230518155848235"><figcaption aria-hidden="true">image-20230518155848235</figcaption></figure><h5 id="方法栈">方法栈</h5><blockquote><p>方法栈是方法的调用流程，下面图就是栈表示，可以说明越下面越是最开始嗲用的，比如Z调用U，U调用(anonymous)，越上面，被调用的层次越深</p></blockquote><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518160347944.png" alt="image-20230518160347944"><figcaption aria-hidden="true">image-20230518160347944</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518191322695.png" alt="image-20230518191322695"><figcaption aria-hidden="true">image-20230518191322695</figcaption></figure><h6 id="找加密数据">找加密数据</h6><blockquote><p>Open是发包初始化,先找open</p></blockquote><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518193036829.png" alt="image-20230518193036829"><figcaption aria-hidden="true">image-20230518193036829</figcaption></figure><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518193124242.png" alt="image-20230518193124242"><figcaption aria-hidden="true">image-20230518193124242</figcaption></figure><blockquote><p>在上面的代码中没有找到相对应的数据，可以到上一个栈中继续寻找</p></blockquote><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/assets\image-20230518193554977.png" alt="image-20230518193554977"><figcaption aria-hidden="true">image-20230518193554977</figcaption></figure><h5 id="参考视频">参考视频</h5><p><a href="https://www.bilibili.com/video/BV1Kh411r7uR/?p=7" class="uri">https://www.bilibili.com/video/BV1Kh411r7uR/?p=7</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AB)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AB)/</url>
    
    <content type="html"><![CDATA[<h4 id="hook">hook</h4><p>链接服务器-&gt;拿回资源-&gt;渲染(解析)资源-&gt;(Js执行流程)初始化(自执行)-&gt;页面逻辑-&gt;等待用户输入-&gt;加密数据-&gt;提交数据</p><p>hook的含义</p><p>在上面流程任意环节中，插入自己的代码，让浏览器先执行自己的代码，然后再执行原本的网站代码</p><p>作用域-&gt;变量所生效的位置</p><p>上下文 = 一个环境（js V8虚拟机）（浏览器 新页面 新线程）</p><p>对于js来说，V8虚拟机就相当于一个新的上下文，对于浏览器来说，新页面和新线程相当于新的上下文</p><hr><p>在不是全局作用域下，写了一个变量，没有用var定义，直接赋值，（先看当前和上级作用域有没有这个变量，有就直接赋值，没有的话给全局作用域定义这个变量并赋值）</p><hr><blockquote><p>this指向问题</p><ul><li>全局作用域 this = window</li><li>方法作用域 this = 调用者</li><li>类的方法里面 this = 类自己</li></ul></blockquote><p>这里就是方法作用域，this = 调用者（location）</p><figure><img src="/2024/07/29/%E7%88%AC%E8%99%AB%E7%AC%AC%E4%BA%8C%E6%AC%A1%E5%AD%A6%E4%B9%A0(%E5%85%AB)/back_up\博客文章\爬虫\assets\image-20230717104219129.png" alt="image-20230717104219129"><figcaption aria-hidden="true">image-20230717104219129</figcaption></figure><p>hook = 改变原方法或原代码执行流程</p><ul><li>覆盖原方法<ul><li>ES6 语法 Object.defineProperty<ul><li>给对象重新定义属性</li><li>监听属性的设置值和获取值</li></ul></li></ul></li><li>Proxy<ul><li>给对象整体监听<ul><li>属性初始化</li><li>设置值和获取值</li><li>构造函数</li></ul></li></ul></li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-title class_">XMLHttpRequest</span>.<span class="hljs-property"><span class="hljs-keyword">prototype</span></span>.<span class="hljs-property">send_</span> = <span class="hljs-title class_">XMLHttpRequest</span>.<span class="hljs-property"><span class="hljs-keyword">prototype</span></span>.<span class="hljs-property">send</span>;<br><br><span class="hljs-title class_">XMLHttpRequest</span>.<span class="hljs-property"><span class="hljs-keyword">prototype</span></span>.<span class="hljs-property">send</span> = <span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;<span class="hljs-keyword">debugger</span>;&#125;<br></code></pre></td></tr></table></figure><p>Hook的时机？？</p><ul><li>只影响hook完成后的操作</li><li>整个浏览器初始化想进行hook<ul><li>（油猴[浏览器插件]，FD插件[代理]）修改原数据</li></ul></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%80)_requests%E3%80%81%E6%9C%89%E8%B6%A3%E5%B0%8F%E8%A7%82%E5%AF%9F/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%80)_requests%E3%80%81%E6%9C%89%E8%B6%A3%E5%B0%8F%E8%A7%82%E5%AF%9F/</url>
    
    <content type="html"><![CDATA[<h4 id="爬虫">爬虫</h4><ul><li>通用爬虫：<ul><li>抓取系统重要组成部分，抓取的是一整张页面数据</li></ul></li><li>聚焦爬虫<ul><li>是建立在通用爬虫的基础之上，抓取的是页面中特定的局部内容</li></ul></li><li>增量式爬虫<ul><li>检测网站中数据更新的情况，只会抓取网站中最新更新出来的数据</li></ul></li></ul><h5 id="反爬机制">反爬机制</h5><p>门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站技术的爬取</p><h5 id="反反爬策略">反反爬策略</h5><p>爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站数据</p><h5 id="robots.txt协议">robots.txt协议：</h5><p>君子协议。规定了万丈中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。</p><p>一般情况下：网站域名+/robots.txt 可以看见robots协议</p><p><a href="https://www.bilibili.com/robots.txt" class="uri">https://www.bilibili.com/robots.txt</a></p><p>可以看出B站禁止所有爬虫爬取网站信息，实际上我们只要爬虫每次请求不要太快，爬取信息不涉及隐私一般不会出问题。</p><p><a href="https://imgse.com/i/xDehUP"><img src="https://s1.ax1x.com/2022/10/17/xDehUP.png" alt="xDehUP.png"></a></p><h4 id="request模块">request模块</h4><p>python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。</p><p>作用：模拟浏览器发请求</p><p>如何使用：（requests模块的编码流程）</p><ul><li>指定url<ul><li>UA伪装</li><li>请求参数的处理</li></ul></li><li>发起请求</li><li>获取响应数据</li><li>持久化存储</li></ul><h5 id="requests环境安装python的解释器为anaconda">requests环境安装（python的解释器为Anaconda）</h5><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript">conda create -n pachong python=<span class="hljs-number">3.6</span> <span class="hljs-comment">#创建conda环境</span><br><span class="hljs-built_in">activate</span> pachong <span class="hljs-comment">#激活conda环境</span><br>pip install requests <span class="hljs-comment">#安装requests库</span><br></code></pre></td></tr></table></figure><p>File-&gt;Setting进入设置页面，根据下图进行添加虚拟conda环境。</p><p><a href="https://imgse.com/i/xDVxZq"><img src="https://s1.ax1x.com/2022/10/17/xDVxZq.png" alt="xDVxZq.png"></a></p><p><a href="https://imgse.com/i/xDZPWF"><img src="https://s1.ax1x.com/2022/10/17/xDZPWF.png" alt="xDZPWF.png"></a></p><p><a href="https://imgse.com/i/xDZAy9"><img src="https://s1.ax1x.com/2022/10/17/xDZAy9.png" alt="xDZAy9.png"></a></p><p>最后点击Apply和OK</p><p><a href="https://imgse.com/i/xDZedx"><img src="https://s1.ax1x.com/2022/10/17/xDZedx.png" alt="xDZedx.png"></a></p><h5 id="requests第一次使用">requests第一次使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 需求：爬取搜狗首页的页面数据</span><br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># step1:指定url</span><br>    url = <span class="hljs-string">&#x27;https://www.sogou.com/&#x27;</span><br>    <span class="hljs-comment"># step2:发起请求</span><br>    <span class="hljs-comment"># get方法会返回一个响应对象</span><br>    response = requests.get(url=url)<br>    <span class="hljs-comment"># step3:获取相应数据，text返回的是字符串形式的相应数据</span><br>    page_text = response.text<br>    <span class="hljs-built_in">print</span>(page_text)<br>    <span class="hljs-comment"># step4:持久化存储</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./sogou.html&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(page_text)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;爬取数据结束!!!&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span><br>如果是直接运行当前的py文件，__name__的值为__main__字符串<br>如果是导入的包，则会是模块名，可以用来在下面进行测试<br></code></pre></td></tr></table></figure><p>最后会得到一个html文件，可以打开看见它的源代码，按住Ctrl + Alt +L可以格式化代码，在html中右键，点击open inbrowser可以查看html样式（没有css样式文件，比较丑）</p><p><a href="https://imgse.com/i/xDZ8OA"><img src="https://s1.ax1x.com/2022/10/17/xDZ8OA.png" alt="xDZ8OA.png"></a></p><h5 id="网页采集器">网页采集器</h5><p>目的：能够在python中输入关键词，爬取输入关键词后的网页</p><p>首先观察下面的链接，可以看出?后面是带的参数，后面应该就是我们需要自己输入的参数，?前面的网址是固定的</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">一般链接?后面都是参数，<span class="hljs-meta">&amp;则用来连接多个参数，很多参数是我们爬虫用不到的，包含自己的请求地址啥的，我们注意选择关键的参数</span><br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/xDZYwt"><img src="https://s1.ax1x.com/2022/10/17/xDZYwt.png" alt="xDZYwt.png"></a></p><p>User-Agent一般按F12后，刷新下界面就可以抓包了，随便点击一个包就能看见User-Agent</p><p><a href="https://imgse.com/i/xDZUFf"><img src="https://s1.ax1x.com/2022/10/17/xDZUFf.png" alt="xDZUFf.png"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 需求：爬取搜狗指定词条对应的搜索结构页面(简单网页采集器)</span><br><span class="hljs-comment"># UA伪装：让爬虫对应的请求载体身份标识伪装成某一款浏览器</span><br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># UA伪装：将对应的User-Agent封装到一个字典中</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    url = <span class="hljs-string">&#x27;https://www.sogou.com/web&#x27;</span><br>    keyword = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;请输入一个搜索词:&#x27;</span>)<br>    param = &#123;<br>        <span class="hljs-string">&#x27;query&#x27;</span>: keyword<br>    &#125;<br>    <span class="hljs-comment">#对指定的url发起的请求对应的url是携带参数的，并且请求过程处理了参数</span><br>    response = requests.get(url=url,params=param,headers=headers)<br><br>    page_text = response.text<br>    fileName = keyword+<span class="hljs-string">&#x27;.html&#x27;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fileName,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(page_text)<br>    <span class="hljs-built_in">print</span>(fileName,<span class="hljs-string">&#x27;保存成功!!!&#x27;</span>)<br></code></pre></td></tr></table></figure><h5 id="百度翻译">百度翻译</h5><p>按F12-&gt;百度翻译输入dog-&gt;点击XHR按钮(包含ajax请求)-&gt;找到有dog参数的包-&gt;找到请求的Url-&gt;找到请求的方式（Post）</p><p><a href="https://imgse.com/i/xDZdfS"><img src="https://s1.ax1x.com/2022/10/17/xDZdfS.png" alt="xDZdfS.png"></a></p><p><a href="https://imgse.com/i/xDeUBR"><img src="https://s1.ax1x.com/2022/10/17/xDeUBR.png" alt="xDeUBR.png"></a></p><p>这里面能够看到响应回来的是什么数据，红框表示是json数据</p><p><a href="https://imgse.com/i/xDeaH1"><img src="https://s1.ax1x.com/2022/10/17/xDeaH1.png" alt="xDeaH1.png"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#1.指定url</span><br>    post_url = <span class="hljs-string">&quot;https://fanyi.baidu.com/sug&quot;</span><br>    <span class="hljs-comment">#2.进行UA伪装</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    <span class="hljs-comment">#3.post请求参数处理(同get请求一致)</span><br>    word = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请输入要翻译的单词:&quot;</span>)<br>    data = &#123;<br>        <span class="hljs-string">&#x27;kw&#x27;</span>:word<br>    &#125;<br>    <span class="hljs-comment">#4.请求发送</span><br>    response = requests.post(url=post_url,data=data,headers=headers)<br>    <span class="hljs-comment">#5.获取相应数据:json()方法返回的是obj(如果确认响应数据是json类型的，才可以使用)</span><br>    dic_obj = response.json()<br><br>    <span class="hljs-comment">#6.持久化存储</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./&#123;&#125;.json&#x27;</span>.<span class="hljs-built_in">format</span>(word),<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        <span class="hljs-comment"># 包含中文，不能使用ascii编码</span><br>        json.dump(obj=dic_obj, fp=fp, ensure_ascii=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;爬取结束！！！&quot;</span>)<br><br></code></pre></td></tr></table></figure><h5 id="豆瓣电影分类排行榜喜剧">豆瓣电影分类排行榜(喜剧)</h5><p>可以看到？后面的参数都可以在请求参数中进行设置，具体参数作用需要分析</p><p><a href="https://imgse.com/i/xDeB4K"><img src="https://s1.ax1x.com/2022/10/17/xDeB4K.png" alt="xDeB4K.png"></a></p><p>看出start可以从0开始</p><p><a href="https://imgse.com/i/xDer9O"><img src="https://s1.ax1x.com/2022/10/17/xDer9O.png" alt="xDer9O.png"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#爬取豆瓣电影分类排行榜</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    url = <span class="hljs-string">&#x27;https://movie.douban.com/j/chart/top_list&#x27;</span><br>    param = &#123;<br>        <span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;24&#x27;</span>,<br>        <span class="hljs-string">&#x27;interval_id&#x27;</span>: <span class="hljs-string">&#x27;100:90&#x27;</span>,<br>        <span class="hljs-string">&#x27;action&#x27;</span>:<span class="hljs-string">&#x27;&#x27;</span>,<br>        <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,<span class="hljs-comment">#从库中第几部电影去取</span><br>        <span class="hljs-string">&#x27;limit&#x27;</span>: <span class="hljs-string">&#x27;20&#x27;</span>,<span class="hljs-comment">#一次取出的个数</span><br>    &#125;<br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    response = requests.get(url=url,params=param,headers=headers)<br><br>    list_data = response.json()<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./douban.json&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        json.dump(list_data,fp=fp,ensure_ascii=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;爬取结束！！！&quot;</span>)<br></code></pre></td></tr></table></figure><h5 id="爬取肯德基餐厅">爬取肯德基餐厅</h5><p><a href="http://www.kfc.com.cn/kfccda/storelist/index.aspx" class="uri">http://www.kfc.com.cn/kfccda/storelist/index.aspx</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#爬取豆瓣电影分类排行榜</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    url = <span class="hljs-string">&#x27;https://movie.douban.com/j/chart/top_list&#x27;</span><br>    param = &#123;<br>        <span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;24&#x27;</span>,<br>        <span class="hljs-string">&#x27;interval_id&#x27;</span>: <span class="hljs-string">&#x27;100:90&#x27;</span>,<br>        <span class="hljs-string">&#x27;action&#x27;</span>:<span class="hljs-string">&#x27;&#x27;</span>,<br>        <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,<span class="hljs-comment">#从库中第几部电影去取</span><br>        <span class="hljs-string">&#x27;limit&#x27;</span>: <span class="hljs-string">&#x27;20&#x27;</span>,<span class="hljs-comment">#一次取出的个数</span><br>    &#125;<br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    response = requests.get(url=url,params=param,headers=headers)<br><br>    list_data = response.json()<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./douban.json&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        json.dump(list_data,fp=fp,ensure_ascii=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;爬取结束！！！&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="快捷键部分已经很熟的快捷键一般不记录">快捷键(部分已经很熟的快捷键一般不记录)</h4><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">Ctrl + Alt + L <span class="hljs-comment">#格式化代码</span><br>按住Ctrl + <span class="hljs-keyword">Shift </span>+ Alt 然后用鼠标左键点击文本，可以让光标在多个位置出现,然后输入一个单引号 <span class="hljs-comment">#可以为选中部分添加单引号</span><br></code></pre></td></tr></table></figure><h4 id="有趣小观察">有趣小观察</h4><p>输入京东(<a href="https://www.jd.com" class="uri">https://www.jd.com</a>)的网址-&gt;按下F12-&gt;按下Console表单-&gt;进入<a href="https://nerv.aotu.io" class="uri">https://nerv.aotu.io</a>网址-&gt;再次按下F12-&gt;Console中输入play-&gt;等待十秒（网站中间的圈圈会转）-&gt;Console中输入stop-&gt;网站的所有内容都会显示</p><p><a href="https://imgse.com/i/xDegud"><img src="https://s1.ax1x.com/2022/10/17/xDegud.png" alt="xDegud.png"></a></p><p><a href="https://imgse.com/i/xDe2DA"><img src="https://s1.ax1x.com/2022/10/17/xDe2DA.png" alt="xDe2DA.png"></a></p><p><a href="https://imgse.com/i/xDeRHI"><img src="https://s1.ax1x.com/2022/10/17/xDeRHI.png" alt="xDeRHI.png"></a></p><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E4%BA%94)_selenium%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E4%BA%94)_selenium%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h4 id="selenium">Selenium</h4><h5 id="selenium的相关使用">selenium的相关使用</h5><p>问题：selenium模块和爬虫之间具有怎样的关联?</p><ul><li>便捷的获取网站中动态加载的数据</li><li>便捷实现模拟登录</li></ul><p>什么是selenium模块?</p><ul><li>基于浏览器自动化的一个模块</li></ul><p>selenium使用流程：</p><ul><li><p>环境安装：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pip install selenium -<span class="hljs-selector-tag">i</span> https:<span class="hljs-comment">//pypi.douban.com/simple</span><br></code></pre></td></tr></table></figure></li><li><p>下载一个浏览器的驱动程序(谷歌浏览器)</p><ul><li><p>下载路径：(似乎校园网不太行)</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">https:</span><span class="hljs-comment">//chromedriver.storage.googleapis.com/index.html</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p>实例化一个浏览器对象</p></li><li><p>编写基于浏览器自动化的操作代码</p><ul><li>发起请求：get(url)</li><li>标签定位：find_element(by='by',value='value')</li><li>标签交互：send_keys('xxx')</li><li>执行js程序：execute_script('jsCode')</li><li>前进，后退：back(),forward()</li><li>关闭浏览器：quit()</li></ul></li><li><p>selenium处理iframe</p><ul><li><p>如果定位的标签存在于iframe标签之中，则必须使用switch_to_frame(id)</p></li><li><p>动作链（拖动）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br></code></pre></td></tr></table></figure><ul><li>实例化一个动作链对象：action = ActionChains(bro)</li><li>click_and_hold(div)：长按并点击操作</li><li>move_by_offset(x,y)：水平移动x距离，垂直移动y距离</li><li>perform()：让动作链立即执行</li><li>action.release()：释放动作链对象</li></ul></li></ul></li></ul><p>视频上的代码实际没跑出来，说lxml中的etree无法导入，网上说是因为文件名创建了lxml的文件，实际没有，也不清楚为什么。（卸载lxml重新安装就好了）<code># http://scxk.nmpa.gov.cn:81/xk/</code>驱动器无法进入，可能是版本不兼容啥的，换个网站就可以了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># http://scxk.nmpa.gov.cn:81/xk/</span><br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><br><span class="hljs-comment"># 实例化一个浏览器对象（传入浏览器的驱动器）</span><br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><br>browser = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>))<br><span class="hljs-comment"># 让浏览器发起一个指定url对应请求</span><br>browser.get(<span class="hljs-string">&#x27;https://www.baidu.com/&#x27;</span>)<br><br><span class="hljs-comment"># page_source获取浏览器当前页面的页面源码数据</span><br>page_text = browser.page_source<br><br><span class="hljs-comment"># 解析出想要的数据</span><br>tree = etree.HTML(page_text)<br>url = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;hotsearch-content-wrapper&quot;]/li[2]/a/@href&#x27;</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(url)<br><br>sleep(<span class="hljs-number">5</span>)<br>browser.quit()<br></code></pre></td></tr></table></figure><h5 id="进入百度搜索种田进入哔哩哔哩回退到百度前进到哔哩哔哩">进入百度，搜索'种田',进入哔哩哔哩，回退到百度，前进到哔哩哔哩</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><br>browser = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>))<br>browser.get(<span class="hljs-string">&#x27;https://www.baidu.com&#x27;</span>)<br><br><span class="hljs-comment"># 标签定位</span><br>search_input = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;kw&#x27;</span>)<br><span class="hljs-comment"># 标签交互</span><br>search_input.send_keys(<span class="hljs-string">&#x27;种田&#x27;</span>)<br><br><span class="hljs-comment"># 点击搜索按钮</span><br>btn = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;su&#x27;</span>)<br>btn.click()<br>sleep(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 执行一组js程序</span><br>browser.execute_script(<span class="hljs-string">&#x27;window.scrollTo(0,document.body.scrollHeight)&#x27;</span>)<br>sleep(<span class="hljs-number">2</span>)<br><br>browser.get(<span class="hljs-string">&quot;https://www.bilibili.com&quot;</span>)<br>sleep(<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 回退</span><br>browser.back()<br>sleep(<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 前进</span><br>browser.forward()<br><br>sleep(<span class="hljs-number">5</span>)<br>browser.quit()<br></code></pre></td></tr></table></figure><h5 id="滑块拖动">滑块拖动</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-comment"># 导入动作链对应的类</span><br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><br>browser = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>))<br>browser.get(<span class="hljs-string">&#x27;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#x27;</span>)<br><br><span class="hljs-comment"># 如果定位的标签存在于ifram标签之中，则必须通过如下操作进行标签定位</span><br>browser.switch_to.frame(<span class="hljs-string">&#x27;iframeResult&#x27;</span>) <span class="hljs-comment"># 切换到浏览器标签定位的作用域</span><br>div = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;draggable&#x27;</span>)<br><br><span class="hljs-comment"># 动作链</span><br>action = ActionChains(browser)<br><span class="hljs-comment"># 点击长按指定的标签</span><br>action.click_and_hold(div)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">8</span>):<br>    <span class="hljs-comment"># perform()立即执行动作链操作</span><br>    <span class="hljs-comment"># move_by_offset(x,y) x:水平方向 y:垂直方向 正表示向右/上，负表示向左/下</span><br>    action.move_by_offset(<span class="hljs-number">35</span>,<span class="hljs-number">0</span>).perform()<br>    sleep(<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment"># 释放动作链,需要添加perform()，不然后面不会弹窗dropped，需要结束动作链操作，估计是松开鼠标</span><br>action.release().perform()<br><br><span class="hljs-built_in">print</span>(div)<br>sleep(<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 关闭浏览器</span><br>browser.quit()<br></code></pre></td></tr></table></figure><h5 id="登录qq空间还有滑块验证未解决">登录qq空间（还有滑块验证未解决）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-comment"># 导入动作链对应的类</span><br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By<br><br>browser = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>))<br>browser.get(<span class="hljs-string">&#x27;https://qzone.qq.com/&#x27;</span>)<br><br>browser.switch_to.frame(<span class="hljs-string">&#x27;login_frame&#x27;</span>)<br>a_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;switcher_plogin&#x27;</span>)<br>a_tag.click()<br><br>userName_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;u&#x27;</span>)<br>password_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;p&#x27;</span>)<br><br>userName_tag.send_keys(<span class="hljs-string">&#x27;1076106158&#x27;</span>)<br>sleep(<span class="hljs-number">1</span>)<br>password_tag.send_keys(<span class="hljs-string">&#x27;xxxx&#x27;</span>)<br>sleep(<span class="hljs-number">1</span>)<br>btn = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;login_button&#x27;</span>)<br>btn.click()<br><br><span class="hljs-built_in">print</span>(browser)<br><br><span class="hljs-comment"># 滑块验证</span><br><br>sleep(<span class="hljs-number">3</span>)<br>browser.quit()<br></code></pre></td></tr></table></figure><h5 id="无头浏览器规避检测">无头浏览器+规避检测</h5><p>规避检测可以参考<a href="https://www.cnblogs.com/loren880898/p/14119457.html" class="uri">https://www.cnblogs.com/loren880898/p/14119457.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-comment"># 实现无可视化界面</span><br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options<br><span class="hljs-comment"># 实现规避检测</span><br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ChromeOptions<br><br><span class="hljs-comment"># 实现无可视化界面的操作</span><br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><br>chrome_options = Options()<br>chrome_options.add_argument(<span class="hljs-string">&#x27;--headless&#x27;</span>)<br>chrome_options.add_argument(<span class="hljs-string">&#x27;--disable-gpu&#x27;</span>)<br><br><span class="hljs-comment"># 实现规避检测</span><br><span class="hljs-comment"># option = ChromeOptions()</span><br><span class="hljs-comment"># option.add_experimental_option(&#x27;excludeSwitches&#x27;,[&#x27;enable-automation&#x27;])</span><br><br><span class="hljs-comment"># 如何实现让selenium规避被检测到的风险,2022-10-09上面的方法已经失效,可以参考https://www.cnblogs.com/loren880898/p/14119457.html</span><br>bro = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>),options=chrome_options)<br><br><span class="hljs-comment"># 无可视化界面（无头浏览器） phantomjs</span><br>bro.get(<span class="hljs-string">&#x27;https://www.baidu.com&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(bro.page_source)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./test.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(bro.page_source)<br>sleep(<span class="hljs-number">2</span>)<br>bro.quit()<br></code></pre></td></tr></table></figure><h5 id="屏幕截屏网页缩放以某个标签为参照物进行点击">屏幕截屏+网页缩放+以某个标签为参照物进行点击</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pip install pillow -i https://pypi.douban.com/simple</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By<br><br><span class="hljs-comment"># 未避免检测，可能会有问题</span><br>bro = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;../chromedriver.exe&#x27;</span>))<br>bro.get(<span class="hljs-string">&#x27;https://kyfw.12306.cn/otn/resources/login.html&#x27;</span>)<br>bro.maximize_window()<br><span class="hljs-comment"># 能将页面向下滑动一个窗口的距离</span><br><span class="hljs-comment"># bro.execute_script(&#x27;window.scrollTo(0,document.body.scrollHeight)&#x27;)</span><br><span class="hljs-comment"># 使用js代码改变窗口比例</span><br>bro.execute_script(<span class="hljs-string">&quot;document.body.style.zoom=&#x27;1&#x27;;&quot;</span>)<br>time.sleep(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment">#save_screenshot将当前页面截图保存</span><br>bro.save_screenshot(<span class="hljs-string">&#x27;./aa.png&#x27;</span>)<br><br><span class="hljs-comment"># 确定验证码图片对应的左上角和右下角的坐标（确定裁剪区域）,截取公众号二维码吧</span><br>gongzhonghao_img = bro.find_element(By.XPATH,<span class="hljs-string">&#x27;//*[@id=&quot;toolbar_Div&quot;]/div[3]/div[1]/ul/li[3]/div/img&#x27;</span>)<br>location = gongzhonghao_img.location<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;location&#x27;</span>,location)<br><br><span class="hljs-comment"># 公众号二维码的大小</span><br>size = gongzhonghao_img.size<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;size&#x27;</span>,size)<br><br><span class="hljs-comment"># 左上角和右下角坐标</span><br>rangle = (<br>    <span class="hljs-built_in">int</span>(location[<span class="hljs-string">&#x27;x&#x27;</span>])*<span class="hljs-number">0.75</span>,<span class="hljs-built_in">int</span>(location[<span class="hljs-string">&#x27;y&#x27;</span>]),<br>    (<span class="hljs-built_in">int</span>(location[<span class="hljs-string">&#x27;x&#x27;</span>])*<span class="hljs-number">0.75</span>+size[<span class="hljs-string">&#x27;width&#x27;</span>]),<span class="hljs-built_in">int</span>(location[<span class="hljs-string">&#x27;y&#x27;</span>])+size[<span class="hljs-string">&#x27;height&#x27;</span>],<br>)<br><br><span class="hljs-comment"># 至此确定了二维码的区域</span><br>i = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./aa.png&#x27;</span>)<br>code_img_name = <span class="hljs-string">&#x27;code.png&#x27;</span><br><span class="hljs-comment"># crop根据指定区域进行裁剪</span><br>frame = i.crop(rangle)<br>frame.save(code_img_name)<br><br><span class="hljs-comment"># 在bro的网页进行动作链操作，以div标签为参照物（即div标签的左上角为(0,0)坐标)进行偏移，再点击该处，立即执行</span><br>ActionChains(bro).move_to_element_with_offset(<span class="hljs-string">&quot;参照物标签&lt;div&gt;啥的&quot;</span>,<span class="hljs-string">&quot;x偏移量&quot;</span>,<span class="hljs-string">&quot;y偏移量&quot;</span>).click().perform()<br></code></pre></td></tr></table></figure><h5 id="selenium连接已存在的浏览器">Selenium连接已存在的浏览器</h5><p><a href="https://www.jiubing.site/pages/c62e69/" class="uri">https://www.jiubing.site/pages/c62e69/</a></p><p><a href="https://blog.csdn.net/xue_11/article/details/125269232" class="uri">https://blog.csdn.net/xue_11/article/details/125269232</a></p><p><a href="https://jiubing.blog.csdn.net/article/details/123603109" class="uri">https://jiubing.blog.csdn.net/article/details/123603109</a></p><p>似乎只能控制第一个开启的浏览器</p><p>具体操作后面补</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-comment"># 导入动作链对应的类</span><br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By<br><span class="hljs-keyword">import</span> subprocess<br>cmd = <span class="hljs-string">&#x27;&quot;C:/Users/lenovo\AppData/Local/Google/Chrome/Application/chrome.exe&quot;&#x27;</span> \<br><span class="hljs-string">&#x27;--remote-debugging-port=9222 &#x27;</span> \<br><span class="hljs-string">&#x27;--user-data-dir=&quot;C:\selenium\ChromeProfile&quot;&#x27;</span><br><br><br>chrome_options = Options()<br>chrome_options.add_experimental_option(<span class="hljs-string">&quot;debuggerAddress&quot;</span>, <span class="hljs-string">&quot;127.0.0.1:9222&quot;</span>)<br><span class="hljs-comment"># 下一行代码会被阻塞，等待自动打开浏览器,即运行下一行代码，应该要用异步来写，现在还不会，先放着吧</span><br>browser = webdriver.Chrome(service=Service(<span class="hljs-string">&#x27;./chromedriver.exe&#x27;</span>),options=chrome_options)<br>subprocess.run(cmd)<br><br><span class="hljs-comment"># browser = webdriver.Chrome(service=Service(&#x27;chromedriver.exe&#x27;))</span><br>browser.get(<span class="hljs-string">&#x27;https://qzone.qq.com/&#x27;</span>)<br><br>sleep(<span class="hljs-number">2</span>)<br>browser.switch_to.frame(<span class="hljs-string">&#x27;login_frame&#x27;</span>)<br>a_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;switcher_plogin&#x27;</span>)<br>a_tag.click()<br><br>sleep(<span class="hljs-number">1</span>)<br>userName_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;u&#x27;</span>)<br>password_tag = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;p&#x27;</span>)<br><br>sleep(<span class="hljs-number">1</span>)<br>userName_tag.send_keys(<span class="hljs-string">&#x27;1076106158&#x27;</span>)<br>sleep(<span class="hljs-number">1</span>)<br>password_tag.send_keys(<span class="hljs-string">&#x27;asd123456789&#x27;</span>)<br>sleep(<span class="hljs-number">1</span>)<br>btn = browser.find_element(by=<span class="hljs-string">&#x27;id&#x27;</span>,value=<span class="hljs-string">&#x27;login_button&#x27;</span>)<br>btn.click()<br><br><span class="hljs-built_in">print</span>(browser.page_source)<br><span class="hljs-comment"># with open(&#x27;./test.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fp:</span><br><span class="hljs-comment">#     fp.write(browser.page_source)</span><br><br>sleep(<span class="hljs-number">3</span>)<br><br><span class="hljs-comment"># 滑块验证</span><br>div = browser.find_element(By.CSS_SELECTOR,<span class="hljs-string">&#x27;tc-fg-item tc-slider-normal&#x27;</span>)<br><span class="hljs-comment"># 动作链</span><br>action = ActionChains(browser)<br><span class="hljs-comment"># 点击长按指定的标签</span><br>action.click_and_hold(div)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>    <span class="hljs-comment"># perform()立即执行动作链操作</span><br>    <span class="hljs-comment"># move_by_offset(x,y) x:水平方向 y:垂直方向 正表示向右/上，负表示向左/下</span><br>    action.move_by_offset(<span class="hljs-number">35</span>,<span class="hljs-number">0</span>).perform()<br>    sleep(<span class="hljs-number">0.5</span>)<br><br>sleep(<span class="hljs-number">3</span>)<br>browser.quit()<br></code></pre></td></tr></table></figure><h4 id="快捷键部分已经很熟的快捷键一般不记录">快捷键(部分已经很熟的快捷键一般不记录)</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">无<br></code></pre></td></tr></table></figure><h4 id="语言">语言</h4><p>Python 3.6</p><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考博客">参考博客</h4><p><a href="https://www.cnblogs.com/loren880898/p/14119457.html" class="uri">https://www.cnblogs.com/loren880898/p/14119457.html</a></p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%89)_%E8%AF%86%E5%88%AB%E7%AE%80%E5%8D%95%E6%95%B0%E5%AD%97%E9%AA%8C%E8%AF%81%E7%A0%81/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%89)_%E8%AF%86%E5%88%AB%E7%AE%80%E5%8D%95%E6%95%B0%E5%AD%97%E9%AA%8C%E8%AF%81%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h4 id="验证码识别">验证码识别</h4><p>封装一个函数，用来识别图片中的文字(使用了ddddocr（带带弟弟ocr），好像是国人写的包)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> ddddocr  <span class="hljs-comment"># 导入 ddddocr</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imageRecognition</span>(<span class="hljs-params">img_path</span>):<br>    ocr = ddddocr.DdddOcr(use_gpu=<span class="hljs-literal">False</span>, device_id=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 实例化</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(img_path, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:  <span class="hljs-comment"># 打开图片</span><br>        img_bytes = f.read()  <span class="hljs-comment"># 读取图片</span><br>        content = ocr.classification(img=img_bytes)  <span class="hljs-comment"># 识别</span><br>        <span class="hljs-comment"># print(&quot;识别到的内容 &#123;&#125;&quot;.format(content))</span><br>        <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-built_in">print</span>(imageRecognition(<span class="hljs-string">&#x27;./test.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure><h5 id="识别古诗文网的验证码">识别古诗文网的验证码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> 图像识别.imageRecognition <span class="hljs-keyword">import</span> imageRecognition<br><span class="hljs-keyword">from</span>  lxml <span class="hljs-keyword">import</span> etree<br><br>url = <span class="hljs-string">&quot;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx&quot;</span><br><br>headers = &#123;<br>    <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>&#125;<br>page_text = requests.get(url=url,headers=headers).text<br><span class="hljs-comment">#解析验证码图片img中的src属性</span><br>tree = etree.HTML(page_text)<br>code_img_src = <span class="hljs-string">&quot;https://so.gushiwen.cn&quot;</span>+tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;imgCode&quot;]/@src&#x27;</span>)[<span class="hljs-number">0</span>]<br>img_data = requests.get(url=code_img_src,headers=headers).content<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./img&#x27;</span>):<br>    os.mkdir(<span class="hljs-string">&#x27;./img&#x27;</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./img/code.jpg&#x27;</span>,<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(img_data)<br><br>code = imageRecognition(<span class="hljs-string">&#x27;./img/code.jpg&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;识别结果是:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(code))<br></code></pre></td></tr></table></figure><p>http/https协议特性:无状态</p><p>没有请求到对应页面数据的原因：</p><pre><code class="hljs">发起的第二次基于个人主页页面请求的时候，服务器端并不知道此请求是基于登录状态下的请求。</code></pre><p>cookie:用来让服务器记录客户端的相关状态</p><ul><li>手动处理：通过抓包同居获取cookie值，将该值封装到headers中。（不建议）</li><li>自动处理：<ul><li>cookie值的来源是哪里？<ul><li>模拟登录post请求后，有服务器端创建</li></ul></li><li>session会话对象：<ul><li>作用<ul><li>可以进行请求的发送</li><li>如果请求过程中产生了cookie，则该cookie会被自动存储/携带在该session对象中</li></ul></li></ul></li><li>创建一个session对象：session = requests.Session()</li><li>使用session随想进行模拟登录post请求的发送（cookie就会被存储在session中）</li><li>session对象对个人主页对应的get请求进行发送（携带了cookie）</li></ul></li></ul><h5 id="登录古诗文网并爬取个人页面">登录古诗文网，并爬取个人页面</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> 图像识别.imageRecognition <span class="hljs-keyword">import</span> imageRecognition<br><span class="hljs-keyword">from</span>  lxml <span class="hljs-keyword">import</span> etree<br><br>url = <span class="hljs-string">&quot;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx&quot;</span><br><br>headers = &#123;<br>    <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>&#125;<br><br><span class="hljs-comment">#需要使用requests.session()来代替requests进行请求，保证登录的时候和验证码获取的是同一时刻，即会话的一致性</span><br>session = requests.session()<br>response = session.get(url=url,headers=headers)<br>response.encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./home_page.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(response.text)<br><span class="hljs-comment">#解析验证码图片img中的src属性</span><br>tree = etree.HTML(response.text)<br><span class="hljs-comment">#或者code_img_src = &quot;https://so.gushiwen.cn&quot;+tree.xpath(&#x27;//*[@id=&quot;imgCode&quot;]/@src&#x27;)[0]</span><br>code_img_src = <span class="hljs-string">&quot;https://so.gushiwen.cn/RandCode.ashx&quot;</span><br>__VIEWSTATE = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;__VIEWSTATE&quot;]/@value&#x27;</span>)[<span class="hljs-number">0</span>]<br>__VIEWSTATEGENERATOR = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;__VIEWSTATEGENERATOR&quot;]/@value&#x27;</span>)[<span class="hljs-number">0</span>]<br><br>img_data = session.get(url=code_img_src,headers=headers).content<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./img&#x27;</span>):<br>    os.mkdir(<span class="hljs-string">&#x27;./img&#x27;</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./img/code.jpg&#x27;</span>,<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(img_data)<br><br>code = imageRecognition(<span class="hljs-string">&#x27;./img/code.jpg&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;识别结果是:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(code))<br><br><span class="hljs-comment">#post请求的发送(模拟登录)</span><br>login_url = <span class="hljs-string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx&#x27;</span><br>data = &#123;<br>    <span class="hljs-string">&#x27;__VIEWSTATE&#x27;</span>: __VIEWSTATE,<br>    <span class="hljs-string">&#x27;__VIEWSTATEGENERATOR&#x27;</span>: __VIEWSTATEGENERATOR,<br>    <span class="hljs-string">&#x27;from&#x27;</span>: <span class="hljs-string">&#x27;http://so.gushiwen.cn/user/collect.aspx&#x27;</span>,<br>    <span class="hljs-string">&#x27;email&#x27;</span>: <span class="hljs-string">&#x27;1324871082@qq.com&#x27;</span>,<br>    <span class="hljs-string">&#x27;pwd&#x27;</span>: <span class="hljs-string">&#x27;douasdfjk;340221&#x27;</span>,<br>    <span class="hljs-string">&#x27;code&#x27;</span>: code,<br>    <span class="hljs-string">&#x27;denglu&#x27;</span>: <span class="hljs-string">&#x27;登录&#x27;</span>,<br>&#125;<br>response = session.post(url=login_url,data=data,headers=headers)<br>response.encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span><br>member_page_text = response.text<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;gushiwang.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(member_page_text)<br></code></pre></td></tr></table></figure><h4 id="代理">代理</h4><p>破解封IP这种反爬机制</p><p>什么是代理：</p><ul><li>代理服务器，将请求发送给代理服务器，代理服务器替客户机进行请求，最后将response返回给真实客户机</li></ul><p>代理的作用：</p><ul><li>突破自身IP访问的限制</li><li>隐藏自身真实IP</li></ul><p>代理相关的网站</p><ul><li>快代理</li><li>西祠代理</li></ul><p>下面的proxies字典能够使用代理IP进行访问（下面代码实测不行，应该需要https的代理IP）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br>url = <span class="hljs-string">&#x27;https://www.baidu.com/s?wd=ip&#x27;</span><br>headers = &#123;<br>    <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>&#125;<br><br>proxies = &#123;<br>    <span class="hljs-string">&#x27;HTTP&#x27;</span>:<span class="hljs-string">&#x27;223.82.60.202:8060&#x27;</span>,<br>&#125;<br>response = requests.get(url=url,headers=headers,proxies=proxies)<br>response.encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span><br><br>page_text = response.text<br><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ip.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>    fp.write(page_text)<br></code></pre></td></tr></table></figure><h4 id="快捷键部分已经很熟的快捷键一般不记录">快捷键(部分已经很熟的快捷键一般不记录)</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">无<br></code></pre></td></tr></table></figure><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%83)_%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB(%E6%9C%AA%E5%AE%8C%E7%BB%93)/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E4%B8%83)_%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB(%E6%9C%AA%E5%AE%8C%E7%BB%93)/</url>
    
    <content type="html"><![CDATA[<h4 id="分布式爬虫">分布式爬虫</h4><ul><li><p>概念：我们需要搭建一个分布式的集群，让其对一组资源进行分布联合爬取</p></li><li><p>作用：提升爬取数据的效率</p></li><li><p>如何实现分布式</p><ul><li><p>安装一个scrapy-redis的组件</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pip install scrapy-redis  -<span class="hljs-selector-tag">i</span> https:<span class="hljs-comment">//pypi.douban.com/simple</span><br></code></pre></td></tr></table></figure></li><li><p>原生的scrapy是不可以实现分布式爬虫，必须要让scrapy结合着scrapy-redis组件一起实现分布式爬虫</p></li><li><p>为什么原生的scrapy不能实现分布式爬取？</p><ul><li>调度器不可以被分布式机群共享</li><li>管道不可以被分布式机群共享</li></ul></li><li><p>scrapy-redis组件作用：</p><ul><li>可以给原生的scrapy框架提供可以被共享的管道和调度器</li></ul></li><li><p>分布式实现流程</p><ul><li><p>创建一个工程</p></li><li><p>创建一个基于CrawlSpider的爬虫文件</p></li><li><p>修改当前的爬虫文件</p><ul><li><p>导包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy_redis.spiders <span class="hljs-keyword">import</span> RedisCrawlSpider<br></code></pre></td></tr></table></figure></li><li><p>将start_urls和allowed_domains进行注释</p></li><li><p>添加一个新属性：redis_key = 'sun'可以被共享的调度器队列名称</p></li><li><p>编写数据相关的操作</p></li><li><p>将当前爬虫类的父类修改成RedisCrawlSpider</p></li></ul></li><li><p>修改配置文件</p><ul><li><p>指定使用可以被共享的管道</p><ul><li><p>指定可以被共享的管道</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs autoit"><span class="hljs-meta">#4.是否将数据存入Redis数据库的管道：</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-meta"># <span class="hljs-string">&#x27;jd.pipelines.JdPipeline&#x27;</span>: 300,</span><br>    <span class="hljs-meta"># <span class="hljs-string">&#x27;jd.pipelines.JDSqlPipeline&#x27;</span>: 300,</span><br>    <span class="hljs-meta">#当开启此管道，该管道将会把数据存到Redis数据库中</span><br>    <span class="hljs-string">&#x27;scrapy.redis.pipelines.RedisPipeline&#x27;</span>:<span class="hljs-number">400</span>,<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>指定可以被共享的调度器</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment">#1.启用调度将请求存储进redis</span><br><span class="hljs-comment"># 使用scrapy-redis组件自己的调度器</span><br><span class="hljs-comment">#from scrapy_redis.scheduler import Scheduler</span><br><span class="hljs-attr">SCHEDULER</span>=<span class="hljs-string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><br><span class="hljs-comment">#2.确保所有spider通过redis共享相同的重复过滤</span><br><span class="hljs-comment"># 增加了一个去重容器类的配置，作用使用Redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化存储</span><br><span class="hljs-comment"># from scrapy_redis.dupefilter import RFPDupeFilter</span><br><span class="hljs-attr">DUPEFILTER_CLASS</span>=<span class="hljs-string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><br><span class="hljs-comment">#3.设置当爬虫结束的时候是否保持redis数据库中的去重集合与任务队列(可以不设置)</span><br><span class="hljs-comment"># 配置调度器是否要持久化，也就是当爬虫结束了，要不要清空Redis中请求队列和去重指纹的set，如果是True，则会继续没有爬过的数据进行爬取</span><br><span class="hljs-attr">SCHEDULER_PERSIST</span>=<span class="hljs-literal">True</span><br><br><span class="hljs-comment">#指定连接到Redis时要使用的主机和端口     目的是连接上redis数据库</span><br><span class="hljs-attr">REDIS_HOST</span>=<span class="hljs-string">&quot;192.168.45.50&quot;</span><br><span class="hljs-attr">REDIS_PORT</span>=<span class="hljs-number">6379</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p>redis相关操作配置</p><ul><li>配置redis的配置文件<ul><li>linux或者mac:redis.conf</li><li>windows:redis.windows.conf</li><li>打开配置文件修改：<ul><li>将bind 127.0.0.1 进行删除</li><li>关闭保护模式：protected-mode yes 改为no</li></ul></li></ul></li><li>结合配置文件开启redis服务<ul><li>redis-server 配置文件位置</li></ul></li><li>启动客户端<ul><li>redis-cli</li></ul></li></ul></li><li><p>执行工程</p><ul><li>scrapy runspider xxx.py</li></ul></li><li><p>向调度器的队列中放入一个起始的url</p><ul><li>调度器的队列在redis 的客户端中<ul><li>lpush xxx www.xxx.com</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><p>fbs.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy_redis.spiders <span class="hljs-keyword">import</span> RedisCrawlSpider<br><br><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">import</span> sys,os<br>sys.path.append(os.path.dirname(os.path.dirname(__file__)))<br><span class="hljs-keyword">from</span> items <span class="hljs-keyword">import</span> Fbspro2Item<br><span class="hljs-keyword">from</span> scrapy.linkextractors <span class="hljs-keyword">import</span> LinkExtractor<br><span class="hljs-keyword">from</span> scrapy.spiders <span class="hljs-keyword">import</span> Rule<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FbsSpider</span>(<span class="hljs-title class_ inherited__">RedisCrawlSpider</span>):<br>    name = <span class="hljs-string">&#x27;fbs&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    <span class="hljs-comment"># start_urls = [&#x27;http://www.xxx.com/&#x27;]</span><br>    <span class="hljs-comment"># 可以被共享的调度器队列的名称</span><br>    redis_key = <span class="hljs-string">&#x27;boke&#x27;</span><br><br>    rules = (<br>        Rule(LinkExtractor(allow=<span class="hljs-string">r&#x27;http://www.wing2791.cn/page/\d+/&#x27;</span>), callback=<span class="hljs-string">&#x27;parse_item&#x27;</span>, follow=<span class="hljs-literal">True</span>),<br>    )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_item</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># 注意：xpath表达式中不可以出现tbody标签，因为tbody标签是浏览器添加的，不是实际代码中出现的</span><br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> article_list:<br>            new_title = article.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>).extract_first()<br>            new_time = article.xpath(<span class="hljs-string">&#x27;./div/div/span[1]/a/time/text()&#x27;</span>).extract_first()<br>            new_author = article.xpath(<span class="hljs-string">&#x27;./div/div/span[2]/span/a/text()&#x27;</span>).extract_first()<br><br>            item = Fbspro2Item()<br>            item[<span class="hljs-string">&#x27;new_title&#x27;</span>] = new_title<br>            item[<span class="hljs-string">&#x27;new_time&#x27;</span>] = new_time<br>            item[<span class="hljs-string">&#x27;new_author&#x27;</span>] = new_author<br><br>            <span class="hljs-keyword">yield</span> item<br><br></code></pre></td></tr></table></figure><p>items.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Fbspro2Item</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    new_title = scrapy.Field()<br>    new_time = scrapy.Field()<br>    new_author = scrapy.Field()<br></code></pre></td></tr></table></figure><p>setting.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for fbsPro2 project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;fbsPro2&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;fbsPro2.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;fbsPro2.spiders&#x27;</span><br><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;fbsPro2.middlewares.Fbspro2SpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;fbsPro2.middlewares.Fbspro2DownloaderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="hljs-comment">#ITEM_PIPELINES = &#123;</span><br><span class="hljs-comment">#    &#x27;fbsPro2.pipelines.Fbspro2Pipeline&#x27;: 300,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br><br><span class="hljs-comment"># 指定管道</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-comment"># &#x27;jd.pipelines.JdPipeline&#x27;: 300,</span><br>    <span class="hljs-comment"># &#x27;jd.pipelines.JDSqlPipeline&#x27;: 300,</span><br>    <span class="hljs-comment">#当开启此管道，该管道将会把数据存到Redis数据库中</span><br>    <span class="hljs-string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>:<span class="hljs-number">400</span>,<br>&#125;<br><br><span class="hljs-comment"># 指定调度器</span><br>DUPEFILTER_CLASS = <span class="hljs-string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br>SCHEDULER=<span class="hljs-string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span><br>SCHEDULER_PERSIST=<span class="hljs-literal">True</span><br><br>REDIS_HOST=<span class="hljs-string">&quot;192.168.45.50&quot;</span><br>REDIS_PORT=<span class="hljs-number">6379</span><br>REDIS_PARAMS = &#123;<br>    <span class="hljs-string">&#x27;password&#x27;</span>: <span class="hljs-string">&#x27;redispassword&#x27;</span>,<br>&#125;<br></code></pre></td></tr></table></figure><p>在虚拟机的redis中输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs redis">redis-server /etc/redis.conf<br>redis-cli<br>auth redispassword<br>keys *<br>lpush boke http://www.wing2791.cn<br>keys *<br></code></pre></td></tr></table></figure><h4 id="快捷键">快捷键</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">无<br></code></pre></td></tr></table></figure><h4 id="语言">语言</h4><p>Python 3.6</p><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E5%85%AD)_scrapy%E6%A1%86%E6%9E%B6/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E5%85%AD)_scrapy%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h4 id="scrapy框架">Scrapy框架</h4><h5 id="scrapy框架的相关概念">scrapy框架的相关概念</h5><ul><li><p>什么是框架？</p><ul><li>就是一个集成了很多功能并且具有很强通用性的一个项目模板</li></ul></li><li><p>如何学习框架？</p><ul><li>专门学习框架封装的各种功能的详细用法</li></ul></li><li><p>什么是scrapy？</p><ul><li>爬虫中封装好的一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式</li></ul></li><li><p>scrapy框架的基本使用</p><ul><li><p>环境的安装</p><ul><li><p>mac or linux:</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> scrapy<br></code></pre></td></tr></table></figure></li><li><p>windows:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pip install scrapy -<span class="hljs-selector-tag">i</span> https:<span class="hljs-comment">//pypi.douban.com/simple</span><br></code></pre></td></tr></table></figure><p>测试：在终端里输入Scrapy,会显示版本然后说怎么使用命令，没有报错就表示安装成功</p></li></ul></li><li><p>创建一个工程：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">scrapy startproject xxxPro</span><br></code></pre></td></tr></table></figure></li><li><p>cd xxxPro</p></li><li><p>在spiders子目录中创建一个爬虫文件</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scrapy </span>genspider spiderName www.xxx.com<br></code></pre></td></tr></table></figure></li><li><p>执行工程：</p><p>需要在自己创建的spiderName目录下运行命令</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">scrapy crawl spiderName</span><br></code></pre></td></tr></table></figure><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">scrapy</span> <span class="hljs-string">crawl</span> <span class="hljs-string">spiderName</span> <span class="hljs-built_in">--nolog</span> <span class="hljs-comment">#不输出日志信息</span><br></code></pre></td></tr></table></figure><p>在spiderName/spiderName目录下有setting.py，添加以下内容</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 修改robots.txt协议为False</span><br><span class="hljs-comment"># Obey robots.txt rules</span><br><span class="hljs-attr">ROBOTSTXT_OBEY</span> = <span class="hljs-literal">False</span><br><span class="hljs-comment"># 显示指定类型的日志信息</span><br><span class="hljs-attr">LOG_LEVEL</span> = <span class="hljs-string">&#x27;ERROR&#x27;</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h5 id="scrapy-框架数据解析">scrapy 框架数据解析</h5><p>b站的评论似乎是js生成的，不会爬，改成使用自己的博客了</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">scrapy startproject bilibiliPro</span><br></code></pre></td></tr></table></figure><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scrapy </span>genspider <span class="hljs-keyword">bilibili </span>www.xxx.com<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> bilibiliPro<br></code></pre></td></tr></table></figure><h6 id="设置user_agentsetting.pyrobots.txt和log_level">设置USER_AGENT(setting.py),robots.txt和LOG_LEVEL</h6><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br><span class="hljs-attribute">USER_AGENT</span> = &#x27;Mozilla/<span class="hljs-number">5</span>.<span class="hljs-number">0</span> (Windows NT <span class="hljs-number">10</span>.<span class="hljs-number">0</span>; Win64; x64) AppleWebKit/<span class="hljs-number">537</span>.<span class="hljs-number">36</span> (KHTML, like Gecko) Chrome/<span class="hljs-number">106.0.0.0</span> Safari/<span class="hljs-number">537</span>.<span class="hljs-number">36</span>&#x27;<br><br><span class="hljs-comment"># Obey robots.txt rules</span><br><span class="hljs-attribute">ROBOTSTXT_OBEY</span> = False<br><br><span class="hljs-attribute">LOG_LEVEL</span> = &#x27;ERROR&#x27;<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;bilibili&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.wing2791.cn/&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;1.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(response.text)<br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> article_list:<br>            <span class="hljs-comment"># xpath返回的是列表，但是列表元素一定是Selector类型的对象</span><br>            <span class="hljs-comment"># extract可以将Selector对象中data参数存储的字符串提取出来</span><br>            user_name = div.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>)[<span class="hljs-number">0</span>].extract()<br>            <span class="hljs-comment"># 和上面的效果一样，表示提取第一个元素</span><br>            <span class="hljs-comment"># user_name = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract_first()</span><br><br>            <span class="hljs-comment"># # 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取了出来，返回的是列表</span><br>            <span class="hljs-comment"># user_name = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract()</span><br>            <span class="hljs-comment"># # 列表提取为字符串</span><br>            <span class="hljs-comment"># user_name = &#x27;&#x27;.join(user_name)</span><br>            <span class="hljs-built_in">print</span>(user_name)<br></code></pre></td></tr></table></figure><h6 id="运行下面的命令启动scrapy框架">运行下面的命令启动scrapy框架</h6><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">scrapy crawl bilibili</span><br></code></pre></td></tr></table></figure><h6 id="scrapy持久化存储">scrapy持久化存储</h6><ul><li><p>scrapy持久化存储：</p><ul><li><p>基于终端命令：只可以将parse方法的返回值存储到本地的文本文件中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scrapy crawl bilibili -o ./boke.csv<br></code></pre></td></tr></table></figure><p>测试代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;bilibili&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.wing2791.cn/&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;1.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(response.text)<br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        all_data = []<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> article_list:<br>            <span class="hljs-comment"># xpath返回的是列表，但是列表元素一定是Selector类型的对象</span><br>            <span class="hljs-comment"># extract可以将Selector对象中data参数存储的字符串提取出来</span><br>            article_title = div.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>)[<span class="hljs-number">0</span>].extract()<br>            <span class="hljs-comment"># 和上面的效果一样，表示提取第一个元素</span><br>            <span class="hljs-comment"># article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract_first()</span><br><br>            <span class="hljs-comment"># # 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取了出来，返回的是列表</span><br>            <span class="hljs-comment"># article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract()</span><br>            <span class="hljs-comment"># # 列表提取为字符串</span><br>            <span class="hljs-comment"># article_title = &#x27;&#x27;.join(article_title)</span><br>            dic = &#123;<br>                <span class="hljs-string">&#x27;article_title&#x27;</span>:article_title<br>            &#125;<br>            all_data.append(dic)<br>        <span class="hljs-keyword">return</span> all_data<br></code></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">crawl: error: Unrecognized output <span class="hljs-keyword">format</span> <span class="hljs-string">&#x27;txt&#x27;</span>. <span class="hljs-keyword">Set</span> a supported one ((<span class="hljs-string">&#x27;json&#x27;</span>, <span class="hljs-string">&#x27;jsonlines&#x27;</span>, <span class="hljs-string">&#x27;jl&#x27;</span>, <span class="hljs-string">&#x27;csv</span><br><span class="hljs-string">&#x27;</span>, <span class="hljs-string">&#x27;xml&#x27;</span>, <span class="hljs-string">&#x27;marshal&#x27;</span>, <span class="hljs-string">&#x27;pickle&#x27;</span>)) <span class="hljs-keyword">after</span> a colon at the <span class="hljs-keyword">end</span> <span class="hljs-keyword">of</span> the output URI (i.e. -o/-O &lt;URI&gt;:&lt;<span class="hljs-keyword">FORMAT</span>&gt;<br>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">as</span> a file <span class="hljs-keyword">extension</span>.<br># 只能存储这些文件，其他的文件格式不支持<br></code></pre></td></tr></table></figure><ul><li>优点：简洁高效便捷</li><li>缺点：局限性比较强（数据只能够存储到指定后缀的文本文件中）</li></ul></li></ul></li></ul><h5 id="管道的使用">管道的使用</h5><ul><li><p>基于管道：</p><ul><li><p>编码流程</p><ul><li><p>数据解析</p></li><li><p>将解析的数据封装存储到item类型的对象</p></li><li><p>将item类型的对象提交给管道进行持久化存储的操作</p></li><li><p>在管道类的process_item中要将其接收的item对象中存储的数据进行持久化存储操作</p></li><li><p>在配置文件中开启管道</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta"># Configure item pipelines</span><br><span class="hljs-meta"># See https:<span class="hljs-comment">//docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br>ITEM_PIPELINES <span class="hljs-punctuation">=</span> &#123;<br>   &#x27;bilibiliPro.pipelines.BilibiliproPipeline&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-meta"># 300表示的是优先级，数值越小，优先级越高，优先级越高，当前类越早被执行</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ul></li><li><p>运行命令和代码：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">scrapy crawl bilibili</span><br></code></pre></td></tr></table></figure></li><li><p>bilibili.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> ..items <span class="hljs-keyword">import</span> BilibiliproItem<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;bilibili&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.wing2791.cn/&#x27;</span>]<br><br>    <span class="hljs-comment"># def parse(self, response):</span><br>    <span class="hljs-comment">#     with open(&#x27;1.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fp:</span><br>    <span class="hljs-comment">#         fp.write(response.text)</span><br>    <span class="hljs-comment">#     article_list = response.xpath(&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;)</span><br>    <span class="hljs-comment">#     all_data = []</span><br>    <span class="hljs-comment">#     for div in article_list:</span><br>    <span class="hljs-comment">#         # xpath返回的是列表，但是列表元素一定是Selector类型的对象</span><br>    <span class="hljs-comment">#         # extract可以将Selector对象中data参数存储的字符串提取出来</span><br>    <span class="hljs-comment">#         article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;)[0].extract()</span><br>    <span class="hljs-comment">#         # 和上面的效果一样，表示提取第一个元素</span><br>    <span class="hljs-comment">#         # article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract_first()</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment">#         # # 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取了出来，返回的是列表</span><br>    <span class="hljs-comment">#         # article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract()</span><br>    <span class="hljs-comment">#         # # 列表提取为字符串</span><br>    <span class="hljs-comment">#         # article_title = &#x27;&#x27;.join(article_title)</span><br>    <span class="hljs-comment">#         dic = &#123;</span><br>    <span class="hljs-comment">#             &#x27;article_title&#x27;:article_title</span><br>    <span class="hljs-comment">#         &#125;</span><br>    <span class="hljs-comment">#         all_data.append(dic)</span><br>    <span class="hljs-comment">#     return all_data</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self,response</span>):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;1.html&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(response.text)<br>            <span class="hljs-comment"># xpath能够用|(或符号)进行多条件判断</span><br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        all_data = []<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> article_list:<br>            <span class="hljs-comment"># xpath返回的是列表，但是列表元素一定是Selector类型的对象</span><br>            <span class="hljs-comment"># extract可以将Selector对象中data参数存储的字符串提取出来</span><br>            article_title = div.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>)[<span class="hljs-number">0</span>].extract()<br>            <span class="hljs-comment"># 和上面的效果一样，表示提取第一个元素</span><br>            <span class="hljs-comment"># article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract_first()</span><br><br>            <span class="hljs-comment"># # 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取了出来，返回的是列表</span><br>            <span class="hljs-comment"># article_title = div.xpath(&#x27;./header/h2/a/text()&#x27;).extract()</span><br>            <span class="hljs-comment"># # 列表提取为字符串</span><br>            <span class="hljs-comment"># article_title = &#x27;&#x27;.join(article_title)</span><br><br>            item = BilibiliproItem()<br>            item[<span class="hljs-string">&#x27;article_title&#x27;</span>] = article_title<br><br>            <span class="hljs-keyword">yield</span> item <span class="hljs-comment">#将item提交给了管道</span><br></code></pre></td></tr></table></figure></li><li><p>items.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliproItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    article_title = scrapy.Field()<br></code></pre></td></tr></table></figure></li><li><p>pipeline.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliproPipeline</span>:<br>    fp = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 重写父类的一个方法，该方法只在开始的爬虫的时候被调用一次</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;开始爬虫....&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./boke.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br><br>    <span class="hljs-comment"># 专门用来处理item类型对象</span><br>    <span class="hljs-comment"># 该方法可以接收爬虫文件提交过来的item对象</span><br>    <span class="hljs-comment"># 该方法每接收到一个item就会被调用一次</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        article_title = item[<span class="hljs-string">&#x27;article_title&#x27;</span>]<br><br>        <span class="hljs-variable language_">self</span>.fp.write(article_title+<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br>        <span class="hljs-keyword">return</span> item<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;结束爬虫&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.fp.close()<br></code></pre></td></tr></table></figure><p>最后会生成一个boke.txt文件</p></li><li><p>好处：</p><ul><li>通用性强</li></ul></li><li><p>缺点：</p><ul><li>编码较为繁琐</li></ul></li></ul></li></ul><p>##### 面试题：将爬取到的数据存储到本地和数据库，如何实现</p><ul><li><p>面试题：将爬取到的数据存储到本地和数据库，如何实现</p><ul><li><p>管道文件中一个管道类对应的是将数据存储到一种平台</p></li><li><p>爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受</p></li><li><p>process_item(self,item,spider)中的returnitem表示将item传输给下一个即将被执行的管道类</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs 1c">ITEM_PIPELINES <span class="hljs-punctuation">=</span> &#123;<br>   &#x27;bilibiliPro.pipelines.BilibiliproPipeline&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>   &#x27;bilibiliPro.pipelines.mysqlPileLine&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">301</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-meta"># 300表示的是优先级，数值越小，优先级越高，优先级越高，当前类越早被执行</span><br>&#125;<br></code></pre></td></tr></table></figure><p>代码：需要先在exercise数据库中创建一个t_article_title表，字段只有一个，article_title(type= varchar(255))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><span class="hljs-keyword">import</span> pymysql<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BilibiliproPipeline</span>:<br>    fp = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 重写父类的一个方法，该方法只在开始的爬虫的时候被调用一次</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;开始爬虫....&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./boke.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br><br>    <span class="hljs-comment"># 专门用来处理item类型对象</span><br>    <span class="hljs-comment"># 该方法可以接收爬虫文件提交过来的item对象</span><br>    <span class="hljs-comment"># 该方法每接收到一个item就会被调用一次</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        article_title = item[<span class="hljs-string">&#x27;article_title&#x27;</span>]<br><br>        <span class="hljs-variable language_">self</span>.fp.write(article_title+<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br>        <span class="hljs-keyword">return</span> item <span class="hljs-comment">#会给传递给下一个即将被执行的管道类</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;结束爬虫&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.fp.close()<br><br><br><span class="hljs-comment"># 管道文件中一个管道类对应将一组数据存储到一个平台或者载体中</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">mysqlPileLine</span>:<br>    conn = <span class="hljs-literal">None</span><br>    cursor = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">open_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-variable language_">self</span>.conn = pymysql.Connect(host=<span class="hljs-string">&#x27;127.0.0.1&#x27;</span>,port=<span class="hljs-number">3306</span>,user=<span class="hljs-string">&quot;root&quot;</span>,password=<span class="hljs-string">&#x27;1111&#x27;</span>,db=<span class="hljs-string">&#x27;exercise&#x27;</span>,charset=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self,item,spider</span>):<br>        <span class="hljs-comment"># 通过连接创建一个对象</span><br>        <span class="hljs-variable language_">self</span>.cursor = <span class="hljs-variable language_">self</span>.conn.cursor()<br><br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-variable language_">self</span>.cursor.execute(<span class="hljs-string">&#x27;insert into t_article_title values(&quot;%s&quot;)&#x27;</span>%(item[<span class="hljs-string">&quot;article_title&quot;</span>]))<br>            <span class="hljs-comment"># sql语句可以试试改成：f&#x27;insert into qiubai values(&quot;&#123;item[&quot;author&quot;]&#125;&quot;,&quot;&#123;item[&quot;content&quot;]&#125;&quot;)&#x27;</span><br>            <span class="hljs-variable language_">self</span>.conn.commit()<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(e)<br>            <span class="hljs-variable language_">self</span>.conn.rollback()<br><br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-variable language_">self</span>.cursor.close()<br>        <span class="hljs-variable language_">self</span>.conn.commit()<br><br><span class="hljs-comment"># 爬虫文件提交的item类型的对象会提交给哪一个管道类？</span><br><span class="hljs-comment"># 只给优先级高的</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h5 id="基于spider的全站数据爬取">基于Spider的全站数据爬取</h5><ul><li><p>基于Spider的全站数据爬取</p><ul><li><p>就是将网站中某板块下的全部页码对应的页面数据进行爬取</p></li><li><p>需求：爬取博客中所有文章的标题</p></li><li><p>实现方式：</p><ul><li><p>将所有页面的url添加到start_url列表（不推荐）</p></li><li><p>自行手动进行请求发送（推荐）</p><ul><li><p>手动请求发送</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">yield</span> scrapy.Request(url,callback) <span class="hljs-comment"># callback专门用作于数据解析</span><br></code></pre></td></tr></table></figure><p>爬取博客中所有文章的标题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BokeSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;boke&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.wing2791.cn&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.wing2791.cn/&#x27;</span>]<br><br>    <span class="hljs-comment">#生成一个ton共的url模板(不可变)</span><br>    url = <span class="hljs-string">&#x27;http://www.wing2791.cn/page/%d/&#x27;</span><br>    page_num = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> article_list:<br>            article_title = article.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>).extract_first()<br>            <span class="hljs-built_in">print</span>(article_title)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.page_num &lt;=<span class="hljs-number">2</span>:<br>            <span class="hljs-variable language_">self</span>.page_num +=<span class="hljs-number">1</span><br>            new_url = <span class="hljs-built_in">format</span>(<span class="hljs-variable language_">self</span>.url%<span class="hljs-variable language_">self</span>.page_num)<br>            <span class="hljs-comment"># 手动请求发送：callback回调函数是专门用于数据解析</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url=new_url,callback=<span class="hljs-variable language_">self</span>.parse)<br></code></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul><h5 id="scrapy-五大组件">Scrapy 五大组件</h5><p><a href="https://imgse.com/i/xDuQAI"><img src="https://s1.ax1x.com/2022/10/17/xDuQAI.png" alt="xDuQAI.png"></a></p><ul><li><p>引擎（Scrapy）</p><ul><li>用来处理整个系统的数据流处理，触发事务（框架核心）</li></ul></li><li><p>调度器（Scheduler）</p><ul><li>用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，可以想象成一个URL（抓取网页的网址或者说是链接）的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址</li></ul></li><li><p>下载器（Downloader）</p><ul><li>用于下载网页内容，并将网页内容返回给蜘蛛（Scrapy下载器是建立在twisted这个高效的异步模型上的）</li></ul></li><li><p>爬虫（Spiders)</p><ul><li>爬虫是主要干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体（item）。用户也可以从中提取出链接，让scrapy继续抓取下一个页面</li></ul></li><li><p>项目管道（Pipeline）</p><ul><li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体，验证实体的有效性，清除不需要的信息，当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li></ul></li><li><p>请求传参</p><ul><li>使用场景：如果爬取解析的数据不再同一张页面中（深度爬取）</li><li>需求：爬取boss的岗位名称，岗位描述</li><li>作用：在Request请求时将item传递给请求所对应的回调函数</li></ul></li></ul><p>实际没跑出来，记录几个要点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># class BossproItem中添加属性</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BossproItem</span>(scrapy.Item):<br>    job_name = scrapy.Field()<br>    job_desc = scrapy.Field()<br><br><span class="hljs-comment"># setting.py汇总解除注释</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;bossPro.pipelines.BossproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># 导入item</span><br><span class="hljs-keyword">from</span> bossPro.items <span class="hljs-keyword">import</span> BossproItem<br><span class="hljs-comment"># parse函数中</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self,response</span>):<br>    item = BossproItem()<br><br>    <span class="hljs-comment"># 请求传参:meta=&#123;&#125;,可以将meta字典传递给请求对应的回调函数</span><br>    <span class="hljs-keyword">yield</span> scrapy.Request(detail_url,callback=<span class="hljs-variable language_">self</span>.parse_detail,meta=&#123;<span class="hljs-string">&#x27;item&#x27;</span>:item,&#125;)<br><br><span class="hljs-comment"># parse_detail函数中</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_detail</span>(<span class="hljs-params">self,response</span>):<br>    <span class="hljs-comment"># 回调函数接受item</span><br>    item = response.meta[<span class="hljs-string">&#x27;item&#x27;</span>]<br>    item[<span class="hljs-string">&#x27;job_desc&#x27;</span>] = job_desc<br></code></pre></td></tr></table></figure><h6 id="图片数据爬取之imagespipeline">图片数据爬取之ImagesPipeline</h6><ul><li><p>基于scrapy爬取字符串类型数据和爬取图片类型的数据区别？</p><ul><li>字符串：只需要基于xpath进行解析且提交管道进行持久化存储</li><li>图片：xpath解析出图片src属性值，单独对图片地址发起请求获取图片二进制类型数据</li></ul></li><li><p>ImagePipeline:</p><ul><li>只需要将img的src属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型数据，且还会帮我们进行持久化存储</li></ul></li><li><p>需求：爬取站长素材中的高清图片</p></li><li><p>结果：爬取的是缩略图，高清大图需要进入网页再次找url，不过主要是学习方法</p></li><li><p>使用流程：</p><ul><li><p>数据解析（图片的地址）---img.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> ..items <span class="hljs-keyword">import</span> ImgsproItem<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ImgSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;img&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://sc.chinaz.com/tupian/&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        div_list = response.xpath(<span class="hljs-string">&#x27;/html/body/div[3]/div[2]/div&#x27;</span>)<br>        <span class="hljs-comment"># 使用伪属性(2022.10.11src同样不会加载，但地址在data-original里面)</span><br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            src = <span class="hljs-string">&#x27;https:&#x27;</span>+div.xpath(<span class="hljs-string">&#x27;./img/@data-original&#x27;</span>).extract_first()<br><br>            item = ImgsproItem()<br>            item[<span class="hljs-string">&#x27;src&#x27;</span>] = src<br><br>            <span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure></li><li><p>将存储图片的地址的item提交到指定的管道类---pipelines.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-comment"># class ImgsproPipeline:</span><br><span class="hljs-comment">#     def process_item(self, item, spider):</span><br><span class="hljs-comment">#         return item</span><br><span class="hljs-keyword">from</span> scrapy.pipelines.images <span class="hljs-keyword">import</span> ImagesPipeline<br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">imgsPileLine</span>(<span class="hljs-title class_ inherited__">ImagesPipeline</span>):<br><br>    <span class="hljs-comment"># 可以根据图片地址进行图片数据的请求</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_media_requests</span>(<span class="hljs-params">self, item, info</span>):<br>        <span class="hljs-keyword">yield</span> scrapy.Request(item[<span class="hljs-string">&#x27;src&#x27;</span>])<br><br>    <span class="hljs-comment"># 指定图片存储的路径</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">file_path</span>(<span class="hljs-params">self, request, response=<span class="hljs-literal">None</span>, info=<span class="hljs-literal">None</span>, *, item=<span class="hljs-literal">None</span></span>):<br>        imgName = request.url.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> imgName<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">item_completed</span>(<span class="hljs-params">self, results, item, info</span>):<br>        <span class="hljs-keyword">return</span> item <span class="hljs-comment"># item会返回给下一个管道类使用</span><br></code></pre></td></tr></table></figure></li><li><p>在管道文件中自定义一个基于ImagesPipeLine的一个管道类（代码如上）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">get_media_requests(<span class="hljs-variable language_">self</span>, item, info)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">file_path</span>(<span class="hljs-params">self, request, response=<span class="hljs-literal">None</span>, info=<span class="hljs-literal">None</span>, *, item=<span class="hljs-literal">None</span></span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">file_path</span>(<span class="hljs-params">self, request, response=<span class="hljs-literal">None</span>, info=<span class="hljs-literal">None</span>, *, item=<span class="hljs-literal">None</span></span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">item_completed</span>(<span class="hljs-params">self, results, item, info</span>)<br></code></pre></td></tr></table></figure></li><li><p>在配置文件中：---setting.py</p><ul><li><p>指定图片存储的目录:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 图片存储仓库</span><br>IMAGES_STORE = <span class="hljs-string">&#x27;./imgs&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>指定开启的管道</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>    <span class="hljs-comment"># 管道名称换成自己自定义的管道类</span><br>   <span class="hljs-string">&#x27;imgsPro.pipelines.imgsPileLine&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>最终配置文件如下：---setting.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for imgsPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;imgsPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;imgsPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;imgsPro.spiders&#x27;</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;imgsPro.middlewares.ImgsproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;imgsPro.middlewares.ImgsproDownloaderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>    <span class="hljs-comment"># 管道名称换成自己自定义的管道类</span><br>   <span class="hljs-string">&#x27;imgsPro.pipelines.imgsPileLine&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br><br><span class="hljs-comment"># 图片存储仓库</span><br>IMAGES_STORE = <span class="hljs-string">&#x27;./imgs&#x27;</span><br></code></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>中间件</p><ul><li>下载中间件<ul><li>位置：引擎和下载器之间</li><li>作用：批量拦截到整个工程中所有的请求和响应</li><li>拦截请求：<ul><li>UA伪装：process_request()</li><li>代理IP：process_exception:return request</li></ul></li><li>拦截响应：<ul><li>篡改响应数据，响应对象</li><li>需求：爬取网易新闻中的新闻数据（标题和内容）<ul><li><ol type="1"><li>通过网易新闻的首页解析出五大板块对应的详情页的url(没有动态加载)</li></ol></li><li><ol start="2" type="1"><li>每一个板块对应的新闻标题都是动态加载出来的（动态加载）</li></ol></li><li><ol start="3" type="1"><li>通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容</li></ol></li></ul></li></ul></li></ul></li></ul></li></ul><p>middle.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MiddleSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;middle&#x27;</span><br>    allowed_domains = [<span class="hljs-string">&#x27;www.xxx.com&#x27;</span>]<br>    start_urls = [<span class="hljs-string">&#x27;https://www.baidu.com/s?wd=ip&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        page_text = response.text()<br><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ip.html&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(page_text)<br></code></pre></td></tr></table></figure><p>setting.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">BOT_NAME = <span class="hljs-string">&#x27;middlePro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;middlePro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;middlePro.spiders&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 开启下载中间件</span><br>DOWNLOADER_MIDDLEWARES = &#123;<br>   <span class="hljs-string">&#x27;middlePro.middlewares.MiddleproDownloaderMiddleware&#x27;</span>: <span class="hljs-number">543</span>,<br>&#125;<br></code></pre></td></tr></table></figure><p>middleware.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> is_item, ItemAdapter<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MiddleproDownloaderMiddleware</span>:<br>    <span class="hljs-comment"># Not all methods need to be defined. If a method is not defined,</span><br>    <span class="hljs-comment"># scrapy acts as if the downloader middleware does not modify the</span><br>    <span class="hljs-comment"># passed objects.</span><br>    user_agent_list = [<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&quot;</span>,<br>    ]<br><br>    PROXY_http = [<br>    ]<br>    PROXY_https = [<br>    ]<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_request</span>(<span class="hljs-params">self, request, spider</span>):<br>        request.headers[<span class="hljs-string">&#x27;User-Agent&#x27;</span>] = random.choice(<span class="hljs-variable language_">self</span>.user_agent_list)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_response</span>(<span class="hljs-params">self, request, response, spider</span>):<br>        <span class="hljs-keyword">return</span> response<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_exception</span>(<span class="hljs-params">self, request, exception, spider</span>):<br>        <span class="hljs-keyword">if</span> request.url.split(<span class="hljs-string">&#x27;:&#x27;</span>)[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;http&#x27;</span>:<br>            <span class="hljs-comment"># 代理</span><br>            request.meta[<span class="hljs-string">&#x27;proxy&#x27;</span>] = <span class="hljs-string">&#x27;http://&#x27;</span> + random.choice(<span class="hljs-variable language_">self</span>.PROXY_http)<br>        <span class="hljs-keyword">else</span>:<br>            request.meta[<span class="hljs-string">&#x27;proxy&#x27;</span>] = <span class="hljs-string">&#x27;httpw://&#x27;</span> + random.choice(<span class="hljs-variable language_">self</span>.PROXY_https)<br>        <span class="hljs-keyword">return</span> request <span class="hljs-comment">#修正之后的请求对象进行重新的请求</span><br></code></pre></td></tr></table></figure><h6 id="网易新闻爬取只爬取两个界面国内-国际">网易新闻爬取（只爬取两个界面，国内-国际）</h6><p>wangyi.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> ..items <span class="hljs-keyword">import</span> WangyiproItem<br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.service <span class="hljs-keyword">import</span> Service<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WangyiSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">&#x27;wangyi&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://news.163.com/&#x27;</span>]<br>    <span class="hljs-comment"># 存储五大板块对应详情页的url</span><br>    models_urls = []<br>    <span class="hljs-comment"># 解析五大板块对应的详情页的url</span><br><br>    <span class="hljs-comment"># 实例化一个浏览器对象</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        option = webdriver.ChromeOptions()<br>        <span class="hljs-comment"># 防止打印一些无用的日志</span><br>        option.add_experimental_option(<span class="hljs-string">&quot;excludeSwitches&quot;</span>, [<span class="hljs-string">&#x27;enable-automation&#x27;</span>, <span class="hljs-string">&#x27;enable-logging&#x27;</span>])<br>        <span class="hljs-variable language_">self</span>.bro = webdriver.Chrome(chrome_options=option,service=Service(<span class="hljs-string">&#x27;chromedriver.exe&#x27;</span>))<br><br>    <span class="hljs-comment"># 解析五大板块对应的</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        li_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;index2016_wrap&quot;]/div[3]/div[2]/div[2]/div[2]/div/ul/li&#x27;</span>)<br>        alist = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]<br>        <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> alist:<br>            model_url = li_list[index].xpath(<span class="hljs-string">&#x27;./a/@href&#x27;</span>).extract_first()<br>            <span class="hljs-variable language_">self</span>.models_urls.append(model_url)<br><br>            <span class="hljs-comment"># 依次对每一个板块对应的页面进行请求</span><br>            <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.models_urls:<span class="hljs-comment"># 对每一个板块的url进行请求发送</span><br>                <span class="hljs-keyword">yield</span> scrapy.Request(url,callback=<span class="hljs-variable language_">self</span>.parse_model)<br><br>    <span class="hljs-comment">#  每一个板块对应的新闻标题相关的内容都是动态加载</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_model</span>(<span class="hljs-params">self,response</span>):<span class="hljs-comment"># 解析每一个板块页面中对应新闻的标题和内容</span><br>        div_list = response.xpath(<span class="hljs-string">&#x27;/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;div_list&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(div_list))<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            title = div.xpath(<span class="hljs-string">&#x27;./div/div[1]/h3/a/text()&#x27;</span>).extract_first()<br>            new_detail_url = div.xpath(<span class="hljs-string">&#x27;./div/div[1]/h3/a/@href&#x27;</span>).extract_first()<br><br>            item = WangyiproItem()<br>            item[<span class="hljs-string">&#x27;title&#x27;</span>] = title<br><br>            <span class="hljs-comment"># 对新闻详情页的url发起请求</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url=new_detail_url,callback=<span class="hljs-variable language_">self</span>.parse_detail,meta=&#123;<span class="hljs-string">&#x27;item&#x27;</span>:item&#125;)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_detail</span>(<span class="hljs-params">self,response</span>): <span class="hljs-comment"># 解析新闻内容</span><br>        content = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div[2]//text()&#x27;</span>).extract()<br>        content = <span class="hljs-string">&#x27;&#x27;</span>.join(content)<br>        item = response.meta[<span class="hljs-string">&#x27;item&#x27;</span>]<br>        item[<span class="hljs-string">&#x27;content&#x27;</span>] = content<br><br>        <span class="hljs-keyword">yield</span> item<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">closed</span>(<span class="hljs-params">self,spider</span>):<br>        <span class="hljs-variable language_">self</span>.bro.quit()<br></code></pre></td></tr></table></figure><p>pipelines.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WangyiproPipeline</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        <span class="hljs-built_in">print</span>(item)<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure><p>settings.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for wangyiPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;wangyiPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;wangyiPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;wangyiPro.spiders&#x27;</span><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;wangyiPro.middlewares.WangyiproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br>DOWNLOADER_MIDDLEWARES = &#123;<br>   <span class="hljs-string">&#x27;wangyiPro.middlewares.WangyiproDownloaderMiddleware&#x27;</span>: <span class="hljs-number">543</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;wangyiPro.pipelines.WangyiproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br></code></pre></td></tr></table></figure><p>items.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WangyiproItem</span>(scrapy.Item):<br>    title = scrapy.Field()<br>    content = scrapy.Field()<br></code></pre></td></tr></table></figure><p>middlewares.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your spider middleware</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> is_item, ItemAdapter<br><span class="hljs-keyword">from</span> scrapy.http <span class="hljs-keyword">import</span> HtmlResponse<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WangyiproDownloaderMiddleware</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_request</span>(<span class="hljs-params">self, request, spider</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 该方法拦截五大板块对应的相应对象，进行篡改</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_response</span>(<span class="hljs-params">self, request, response, spider</span>):<br>        <span class="hljs-comment"># 获取在爬虫类中定义的浏览器对象</span><br>        bro = spider.bro<br><br>        <span class="hljs-comment"># 挑选出指定的响应对象进行篡改</span><br>        <span class="hljs-comment"># 通过url指定request</span><br>        <span class="hljs-comment"># 通过request指定response</span><br>        <span class="hljs-keyword">if</span> request.url <span class="hljs-keyword">in</span> spider.models_urls:<br>            bro.get(request.url) <span class="hljs-comment"># 对五个板块对应的的url进行请求</span><br>            sleep(<span class="hljs-number">3</span>)<br>            page_text = bro.page_source <span class="hljs-comment"># 获取动态加载的页面数据</span><br><br>            <span class="hljs-comment"># response  # 五大板块对应的响应对象</span><br>            <span class="hljs-comment"># 针对定位到的这些response进行篡改</span><br>            <span class="hljs-comment"># 实例化一个新的响应对象（符合需求：包含动态加载出的新闻数据），替代原来旧的响应对象</span><br>            <span class="hljs-comment"># 如何获取动态加载出的新闻数据</span><br>            <span class="hljs-comment"># url:响应对象所对应请求的url</span><br>            <span class="hljs-comment"># body:接响应数据</span><br>            <span class="hljs-comment"># encoding:编码格式</span><br>            <span class="hljs-comment"># request:响应对象对应的请求对象</span><br>            new_response = HtmlResponse(url=request.url,body=page_text,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>,request=request)<br><br>            <span class="hljs-keyword">return</span> new_response<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># response #其他请求对应的响应对象</span><br>            <span class="hljs-keyword">return</span> response<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_exception</span>(<span class="hljs-params">self, request, exception, spider</span>):<br>        <span class="hljs-comment"># Called when a download handler or a process_request()</span><br>        <span class="hljs-comment"># (from other downloader middleware) raises an exception.</span><br><br>        <span class="hljs-comment"># Must either:</span><br>        <span class="hljs-comment"># - return None: continue processing this exception</span><br>        <span class="hljs-comment"># - return a Response object: stops process_exception() chain</span><br>        <span class="hljs-comment"># - return a Request object: stops process_exception() chain</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><h5 id="crawlspider全站数据爬取">CrawlSpider全站数据爬取</h5><ul><li>全站数据爬取的方式<ul><li>基于Spider：手动请求</li><li>基于CrawlSpider</li></ul></li><li>CrawlSpider的使用<ul><li>创建一个工程 scrapy startproject xxx</li><li>cd xxx</li><li>创建爬虫文件（CrawlSpider）：<ul><li>scrapy genspider -t crawl xxx www.xxx.com</li><li>链接提取器：<ul><li>作用：根据指定的规则（allow）进行指定链接的提取</li></ul></li><li>规则解析器<ul><li>作用：将链接提取器提取到的链接进行指定规则（callback）的解析</li></ul></li></ul></li></ul></li></ul><h6 id="全站爬取博客">全站爬取博客</h6><p>运行在sunPro文件夹下输入<code>scrapy crawl sun</code></p><p>sun.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> scrapy.linkextractors <span class="hljs-keyword">import</span> LinkExtractor<br><span class="hljs-keyword">from</span> scrapy.spiders <span class="hljs-keyword">import</span> CrawlSpider, Rule<br><span class="hljs-keyword">from</span> ..items <span class="hljs-keyword">import</span> SunproItem,DetailItem<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SunSpider</span>(<span class="hljs-title class_ inherited__">CrawlSpider</span>):<br>    name = <span class="hljs-string">&#x27;sun&#x27;</span><br>    <span class="hljs-comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    <span class="hljs-comment"># 这下面写的是主页面url</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.wing2791.cn/&#x27;</span>]<br><br>    <span class="hljs-comment"># 链接提取器：根据指定规则(allow=&quot;正则&quot;)进行指定链接的提取</span><br>    <span class="hljs-comment"># 一个链接提取器对应一个规则解析器</span><br>    link = LinkExtractor(allow=<span class="hljs-string">r&#x27;http://www.wing2791.cn/page/\d+/&#x27;</span>)<br>    <span class="hljs-comment"># http://www.wing2791.cn/2022/07/04/%e7%bd%91%e7%ab%99%e4%bf%a1%e6%81%af%e9%80%9f%e8%af%bb/</span><br>    <span class="hljs-comment"># http://www.wing2791.cn/2022/09/24/java_%e6%9e%9a%e4%b8%be/</span><br>    link_detail = LinkExtractor(allow=<span class="hljs-string">r&#x27;www.wing2791.cn/\d+/\d+/\d+/.*?/&#x27;</span>)<br>    rules = (<br>        <span class="hljs-comment"># 规则解析器：将链接提取器提取到的链接进行指定规则(callback)的解析操作</span><br>        Rule(link, callback=<span class="hljs-string">&#x27;parse_item&#x27;</span>, follow=<span class="hljs-literal">True</span>),<br>        <span class="hljs-comment"># follow=True:可以将链接提取器继续作用到连接提取器提取到的链接所对应的页面中</span><br>        Rule(link_detail,callback=<span class="hljs-string">&#x27;parse_detail&#x27;</span>)<br>    )<br><br>    <span class="hljs-comment"># 解析标题，时间和作者</span><br>    <span class="hljs-comment"># 如下两个解析方法是不可以实现请求传参，因为没有手动request请求</span><br>    <span class="hljs-comment"># 如果将两个解析方法解析的数据存储到同一个item中可以依次存储到两个item中</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_item</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># 注意：xpath表达式中不可以出现tbody标签，因为tbody标签是浏览器添加的，不是实际代码中出现的</span><br>        article_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/main/article&#x27;</span>)<br>        <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> article_list:<br>            new_title = article.xpath(<span class="hljs-string">&#x27;./header/h2/a/text()&#x27;</span>).extract_first()<br>            new_time = article.xpath(<span class="hljs-string">&#x27;./div/div/span[1]/a/time/text()&#x27;</span>).extract_first()<br>            new_author = article.xpath(<span class="hljs-string">&#x27;./div/div/span[2]/span/a/text()&#x27;</span>).extract_first()<br>            item = SunproItem()<br>            item[<span class="hljs-string">&#x27;new_title&#x27;</span>] = new_title<br>            item[<span class="hljs-string">&#x27;new_time&#x27;</span>] = new_time<br>            item[<span class="hljs-string">&#x27;new_author&#x27;</span>] = new_author<br>            <span class="hljs-comment"># print(new_title,new_time,new_author)</span><br>            <span class="hljs-keyword">yield</span> item<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_detail</span>(<span class="hljs-params">self,response</span>):<br>        new_content = response.xpath(<span class="hljs-string">&#x27;//article/div//text()&#x27;</span>).extract()<br>        new_content = <span class="hljs-string">&#x27;&#x27;</span>.join(new_content)<br>        item = DetailItem()<br>        item[<span class="hljs-string">&#x27;new_content&#x27;</span>] = new_content<br>        <span class="hljs-comment"># print(new_content)</span><br>        <span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure><p>items.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-comment"># 进行持久化存储</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SunproItem</span>(scrapy.Item):<br>    new_title = scrapy.Field()<br>    new_time = scrapy.Field()<br>    new_author = scrapy.Field()<br><br><br><span class="hljs-comment"># 进行持久化存储</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DetailItem</span>(scrapy.Item):<br>    new_content = scrapy.Field()<br></code></pre></td></tr></table></figure><p>setting.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for sunPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;sunPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;sunPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;sunPro.spiders&#x27;</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;sunPro.pipelines.SunproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br></code></pre></td></tr></table></figure><p>pipeline.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SunproPipeline</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        <span class="hljs-comment"># 如何判定item的类型</span><br>        <span class="hljs-comment"># 将数据写入数据库时如何保证数据的一致性，连接查询？？找主键？建两个表，使用外键约束？</span><br>        <span class="hljs-keyword">if</span> item.__class__.__name__ == <span class="hljs-string">&#x27;DetailItem&#x27;</span>:<br>            <span class="hljs-built_in">print</span>(item[<span class="hljs-string">&#x27;new_content&#x27;</span>])<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(item[<span class="hljs-string">&#x27;new_title&#x27;</span>],item[<span class="hljs-string">&#x27;new_time&#x27;</span>],item[<span class="hljs-string">&#x27;new_author&#x27;</span>])<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure><h4 id="快捷键">快捷键</h4><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">Ctrl</span> + L <span class="hljs-comment">#清空终端</span><br><span class="hljs-comment"># 按住滚轮选中多行可以并行输入</span><br></code></pre></td></tr></table></figure><h4 id="语言">语言</h4><p>Python 3.6</p><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考博客">参考博客</h4><p><a href="https://www.cnblogs.com/loren880898/p/14119457.html" class="uri">https://www.cnblogs.com/loren880898/p/14119457.html</a></p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/07/29/%E7%88%AC%E8%99%AB(%E4%BA%8C)_%E6%AD%A3%E5%88%99%E3%80%81bs4%E3%80%81xpath/"/>
    <url>/2024/07/29/%E7%88%AC%E8%99%AB(%E4%BA%8C)_%E6%AD%A3%E5%88%99%E3%80%81bs4%E3%80%81xpath/</url>
    
    <content type="html"><![CDATA[<h4 id="聚焦爬虫">聚焦爬虫</h4><p>爬取页面中指定的页面内容</p><ul><li>指定url</li><li>发起请求</li><li>获取响应数据</li><li>数据解析</li><li>持久化存储</li></ul><h5 id="数据解析分类">数据解析分类：</h5><ul><li>正则</li><li>bs4</li><li>xpath</li></ul><h5 id="数据解析原理概述">数据解析原理概述</h5><ul><li>解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储<ul><li>进行指定标签的定位</li><li>标签或者标签对应的属性中存储的数据值进行提取(解析)</li></ul></li></ul><h5 id="常见正则表达式">常见正则表达式</h5><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">单字符</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">.</span><span class="hljs-punctuation">:</span> <span class="hljs-string">除换行以外的所有字符</span><br><span class="hljs-attribute">[]</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[aoe] [a-w] 匹配集合中任意一个字符</span><br><span class="hljs-attribute">\d</span><span class="hljs-punctuation">:</span> <span class="hljs-string">数字 [0-9]</span><br><span class="hljs-attribute">\D</span><span class="hljs-punctuation">:</span> <span class="hljs-string">非数字</span><br><span class="hljs-attribute">\w</span><span class="hljs-punctuation">:</span> <span class="hljs-string">数字、字母、下划线、中文</span><br><span class="hljs-attribute">\W</span><span class="hljs-punctuation">:</span> <span class="hljs-string">非数字、非字母、非下划线、非中文</span><br><span class="hljs-attribute">\s</span><span class="hljs-punctuation">:</span> <span class="hljs-string">所有的空白字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]</span><br><span class="hljs-attribute">\S</span><span class="hljs-punctuation">:</span> <span class="hljs-string">非空白</span><br><span class="hljs-attribute">数量修饰</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">*</span><span class="hljs-punctuation">:</span> <span class="hljs-string">任意多次 &gt;=0</span><br><span class="hljs-attribute">+</span><span class="hljs-punctuation">:</span> <span class="hljs-string">至少1次 &gt;=1</span><br><span class="hljs-attribute">?</span><span class="hljs-punctuation">:</span> <span class="hljs-string">可有可无 0次或者1次</span><br><span class="hljs-attribute">&#123;m&#125;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">固定m次</span><br><span class="hljs-attribute">&#123;m,&#125;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">至少m次</span><br><span class="hljs-attribute">&#123;m,n&#125;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">m-n次</span><br><span class="hljs-attribute">边界</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">$</span><span class="hljs-punctuation">:</span> <span class="hljs-string">以某某结尾</span><br><span class="hljs-attribute">^</span><span class="hljs-punctuation">:</span> <span class="hljs-string">以某某开头</span><br><span class="hljs-attribute">分组</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">(ab)</span><br><span class="hljs-attribute">贪婪模式</span><span class="hljs-punctuation">:</span> <span class="hljs-string">.*</span><br><span class="hljs-attribute">非贪婪(惰性)模式</span><span class="hljs-punctuation">:</span> <span class="hljs-string">.*?</span><br><span class="hljs-attribute">re.I</span><span class="hljs-punctuation"> :</span> <span class="hljs-string">忽略大小写</span><br><span class="hljs-attribute">re.M</span><span class="hljs-punctuation"> :</span> <span class="hljs-string">多行匹配</span><br><span class="hljs-attribute">re.S</span><span class="hljs-punctuation"> :</span> <span class="hljs-string">单行匹配</span><br><br>re.sub(正则表达式,替换内容,字符串)<br></code></pre></td></tr></table></figure><h5 id="图片数据爬取">图片数据爬取</h5><p>爬取指定博客网站中的图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./wing2791Libs&#x27;</span>):<br>        os.mkdir(<span class="hljs-string">&#x27;./wing2791Libs&#x27;</span>)<br><br>    url = <span class="hljs-string">&#x27;http://www.wing2791.cn/2022/09/13/ps%e5%9f%ba%e7%a1%80%e5%ad%a6%e4%b9%a0%ef%bc%88%e4%b8%89%ef%bc%89/&#x27;</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    page_text = requests.get(url=url,headers=headers).text<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./wing2791Libs/page.html&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(page_text)<br>    ex = <span class="hljs-string">&#x27;&lt;p&gt;&lt;a .*?&gt;&lt;img src=&quot;(.*?)&quot; alt.*?&gt;&lt;/a&gt;&lt;/p&gt;&#x27;</span><br><br>    img_src_list = re.findall(ex,page_text,re.S)<br><br>    <span class="hljs-keyword">for</span> src <span class="hljs-keyword">in</span> img_src_list:<br>        img_data = requests.get(url=src,headers=headers).content<br>        fileName = src.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./wing2791Libs/&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(fileName), <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(img_data)<br>            sleep(<span class="hljs-number">0.5</span>)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;下载成功!!!&quot;</span>.<span class="hljs-built_in">format</span>(fileName))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;图片爬取结束!!!&quot;</span>)<br></code></pre></td></tr></table></figure><p>爬取博客中的所有图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./wing2791Libs&#x27;</span>):<br>        os.mkdir(<span class="hljs-string">&#x27;./wing2791Libs&#x27;</span>)<br><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;</span><br>    &#125;<br><br>    main_url_format = <span class="hljs-string">&#x27;http://www.wing2791.cn/page/%d&#x27;</span><br>    pageNumber = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请输入要爬取的总页数:&quot;</span>)<br>    <span class="hljs-keyword">for</span> pageNum <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">int</span>(pageNumber)+<span class="hljs-number">1</span>):<br>        main_url = <span class="hljs-built_in">format</span>(main_url_format%pageNum)<br>        main_page_text = requests.get(url=main_url,headers=headers).text<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./wing2791Libs/main_page_&#123;&#125;.html&quot;</span>.<span class="hljs-built_in">format</span>(pageNum),<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(main_page_text)<br>        main_ex = <span class="hljs-string">&#x27;&lt;h2 class=&quot;entry-title&quot;&gt;&lt;a href=&quot;(.*?)&quot; .*?&gt;.*?&lt;/a&gt;&lt;/h2&gt;&#x27;</span><br>        main_img_src_list = re.findall(main_ex,main_page_text,re.S)<br>        <span class="hljs-keyword">for</span> main_url <span class="hljs-keyword">in</span> main_img_src_list:<br>            page_text = requests.get(url=main_url,headers=headers).text<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./wing2791Libs/page.html&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>                fp.write(page_text)<br>            ex = <span class="hljs-string">&#x27;&lt;p&gt;&lt;a .*?&gt;&lt;img src=&quot;(.*?)&quot; alt.*?&gt;&lt;/a&gt;&lt;/p&gt;&#x27;</span><br><br>            img_src_list = re.findall(ex,page_text,re.S)<br><br>            <span class="hljs-keyword">for</span> src <span class="hljs-keyword">in</span> img_src_list:<br>                img_data = requests.get(url=src,headers=headers).content<br>                fileName = src.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./wing2791Libs/&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(fileName), <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>                    fp.write(img_data)<br>                    sleep(<span class="hljs-number">0.5</span>)<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;下载成功!!!&quot;</span>.<span class="hljs-built_in">format</span>(fileName))<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;图片爬取结束!!!&quot;</span>)<br></code></pre></td></tr></table></figure><h5 id="bs4进行数据解析">bs4进行数据解析</h5><ul><li><p>数据解析的原理</p><ul><li>标签定位</li><li>提取标签，标签属性中存储的数据值</li></ul></li><li><p>bs4数据解析的原理</p><ul><li>实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中</li><li>通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取</li></ul></li><li><p>环境安装</p><ul><li>```python pip install bs4 -i https://pypi.douban.com/simple<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>- ```python<br><span class="hljs-title">  pip install lxml  -i https:</span>//pypi.douban.com/simple<br></code></pre></td></tr></table></figure></li></ul></li><li><p>如何实例化BeautifulSoup对象：</p><ul><li><p>```python from bs4 import BeautifulSoup <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><br>- 对象的实例化<br><br>  - 将本地的html文档中的数据加载到该对象中<br><br>    ```python<br>    fp = <span class="hljs-keyword">open</span>(<span class="hljs-string">&#x27;./test.html&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>    soup = BeautifulSoup(fp,<span class="hljs-string">&#x27;lxml&#x27;</span>)<br></code></pre></td></tr></table></figure></p><ul><li><p>将互联网上获取的页面源码加载到该对象中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">page_text = response.text<br>soup = BeautifulSoup(page_text,<span class="hljs-string">&#x27;lxml&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul></li><li><p>提供的用于数据解析的方法和属性:</p><ul><li>soup.tagName:返回的是文档中第一次出现的tagName对应的标签</li><li>soup.find():<ul><li>find('tagName'):等同于soup.div</li><li>属性定位：<ul><li>soup.find('div',class_/id/attr='song')</li></ul></li></ul></li><li>soup.find_all('tagName'):返回符合要求的所有标签（返回的数据类型是列表）</li></ul></li><li><p>select：</p><ul><li>select('某种选择器（id,class,标签...选择器)'),返回的是一个列表</li></ul></li><li><p>层级选择器</p><ul><li>soup.select('.tang &gt; ul &gt;li &gt; a'): &gt; 表示一个层级</li><li>soup.select('.tang&gt; ul a'): 空格表示多个层级</li></ul></li><li><p>获取标签之间的文本数据：</p><ul><li>soup.a.text/string/get_text()<ul><li>text/get_text():可以获取某一个标签中所有的文本内容</li><li>string:只可以获取该标签下面直系的文本内容</li></ul></li></ul></li><li><p>获取标签中属性值：</p><ul><li>soup.a['href']</li></ul></li></ul></li></ul><h6 id="beautiful解析三国演义">Beautiful解析三国演义</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">import</span> lxml<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    url = <span class="hljs-string">&quot;https://www.shicimingju.com/book/sanguoyanyi.html&quot;</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    response = requests.get(url=url,headers=headers)<br>    <span class="hljs-comment">#Beautiful爬取utf-8编码的网页会乱码，需要设置response的编码格式</span><br>    response.encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span><br>    page_text = response.text<br>    soup = BeautifulSoup(page_text,<span class="hljs-string">&#x27;lxml&#x27;</span>)<br>    li_list = soup.select(<span class="hljs-string">&#x27;.book-mulu &gt; ul &gt; li&#x27;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./sanguo.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            title = li.a.string<br>            <span class="hljs-comment">#title = title.encode(&#x27;iso-8859-1&#x27;).decode(&#x27;gbk&#x27;)</span><br>            detail_url = <span class="hljs-string">&#x27;https://www.shicimingju.com&#x27;</span>+li.a[<span class="hljs-string">&#x27;href&#x27;</span>]<br>            detail_reponse = requests.get(url=detail_url,headers=headers)<br>            detail_reponse.encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span><br>            detail_page_text = detail_reponse.text<br>            detail_soup = BeautifulSoup(detail_page_text,<span class="hljs-string">&#x27;lxml&#x27;</span>)<br>            detail_div = detail_soup.find(<span class="hljs-string">&#x27;div&#x27;</span>,class_=<span class="hljs-string">&#x27;chapter_content&#x27;</span>)<br>            content = detail_div.text<br>            fp.write(title+<span class="hljs-string">&#x27;:&#x27;</span>+content+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>            <span class="hljs-built_in">print</span>(title,<span class="hljs-string">&#x27;爬取成功!&#x27;</span>)<br></code></pre></td></tr></table></figure><h5 id="xpath解析">xpath解析</h5><p>最常用且最便捷高效的一种解析方式，通用性</p><ul><li><p>xpath解析原理：</p><ul><li>实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中</li><li>调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获</li></ul></li><li><p>环境的安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> lxml<br></code></pre></td></tr></table></figure></li><li><p>如何实例化一个etree对象:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br></code></pre></td></tr></table></figure><ul><li><p>将本地的html文档中的源码数据加载到etree对象中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">etree.parse(filePath)<br></code></pre></td></tr></table></figure></li><li><p>可以将互联网上获取的源码数据加载到该对象中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">etree.HTML(<span class="hljs-string">&#x27;page_text&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>xpath('xpath表达式')</p></li></ul></li></ul><h6 id="xpath表达式">xpath表达式</h6><ul><li><h6 id="语法">语法</h6><ul><li>/：表示的是从根节点开始定位，也可以表示一个层级</li><li>//：表示的是多个层级，也可以表示从任意位置开始定位</li><li>属性定位：//div<span class="citation" data-cites="class">[@class='song']</span> tag<span class="citation" data-cites="attrName">[@attrName="attrValue"]</span></li><li>索引定位：//div<span class="citation" data-cites="class">[@class="song"]</span>/p[3] 索引是从1开始</li><li>取文本<ul><li>/text() 获取的是标签中直系的文本内容</li><li>//text()获取的是非直系的文本内容(所有的文本内容)</li></ul></li><li>取属性<ul><li>标签/<span class="citation" data-cites="attrName">@attrName</span>==&gt;img/src</li></ul></li></ul></li></ul><h6 id="xpath解析博客标题">xpath解析博客标题</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    url = <span class="hljs-string">&quot;http://www.wing2791.cn/&quot;</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    response = requests.get(url=url,headers=headers)<br>    page_text = response.text<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./test.txt&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        <span class="hljs-comment">#数据解析</span><br>        tree = etree.HTML(page_text)<br>        <span class="hljs-comment">#xpath解析时能够使用 | 来连接多个xpath表达式，可以匹配多种情况</span><br>        a_list = tree.xpath(<span class="hljs-string">&#x27;//header[@class=&quot;entry-header&quot;]/h2/a&#x27;</span>)<br>        <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> a_list:<br>            <span class="hljs-comment">#局部分析</span><br>            title = a.xpath(<span class="hljs-string">&#x27;./text()&#x27;</span>)[<span class="hljs-number">0</span>]<br>            <span class="hljs-built_in">print</span>(title)<br>            fp.write(title+<span class="hljs-string">&#x27;\n&#x27;</span>)<br></code></pre></td></tr></table></figure><h6 id="xpath下载简历">xpath下载简历</h6><p>主网站：<a href="https://sc.chinaz.com/jianli/index.html" class="uri">https://sc.chinaz.com/jianli/index.html</a> 或者 <a href="https://sc.chinaz.com/jianli/free.html" class="uri">https://sc.chinaz.com/jianli/free.html</a></p><p>如果要爬取多个页面的简历，改一下最初的url，多遍历几轮就行（第二页的链接为<a href="https://sc.chinaz.com/jianli/index_2.html" class="uri">https://sc.chinaz.com/jianli/index_2.html</a>）,第一页比较特殊，没有_1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep<br><br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    url = <span class="hljs-string">&quot;https://sc.chinaz.com/jianli/free.html&quot;</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.176.400 QQBrowser/11.2.5166.400&#x27;</span><br>    &#125;<br>    response = requests.get(url=url, headers=headers)<br>    page_text = response.text<br><br>    tree = etree.HTML(page_text)<br>    href_list = tree.xpath(<span class="hljs-string">&#x27;//div[@id=&quot;main&quot;]/div[@id=&quot;container&quot;]/div/a/@href&#x27;</span>)<br>    <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> href_list:<br>        jianli_url = <span class="hljs-string">&quot;https:&quot;</span> + h<br>        fileName = (jianli_url.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>:<span class="hljs-number">12</span>]<br><br>        jianli_page_text = requests.get(url=jianli_url,headers=headers).text<br>        jianli_tree = etree.HTML(jianli_page_text)<br>        jianli_file_tree = jianli_tree.xpath(<span class="hljs-string">&#x27;//div[@class=&quot;down_wrap&quot;]//ul[@class=&quot;clearfix&quot;]/li/a/@href&#x27;</span>)<br><br>        jianli_data = requests.get(url=jianli_file_tree[<span class="hljs-number">0</span>],headers=headers).content<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;./jianli&#x27;</span>):<br>            os.mkdir(<span class="hljs-string">&#x27;./jianli&#x27;</span>)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./jianli/&#123;&#125;.rar&#x27;</span>.<span class="hljs-built_in">format</span>(fileName),<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(jianli_data)<br>            sleep(<span class="hljs-number">0.2</span>)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;下载成功&quot;</span>.<span class="hljs-built_in">format</span>(fileName))<br></code></pre></td></tr></table></figure><h4 id="快捷键部分已经很熟的快捷键一般不记录">快捷键(部分已经很熟的快捷键一般不记录)</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">无<br></code></pre></td></tr></table></figure><h4 id="编译器">编译器</h4><p>Pycharm</p><h4 id="参考视频">参考视频</h4><p><a href="https://www.bilibili.com/video/BV1Yh411o7Sz" title="参考B站‘路飞学城IT’视频">参考视频链接</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>杂谈-游戏(一)</title>
    <link href="/2024/07/29/%E6%9D%82%E8%B0%88-%E6%B8%B8%E6%88%8F%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2024/07/29/%E6%9D%82%E8%B0%88-%E6%B8%B8%E6%88%8F%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="游戏杂谈">游戏杂谈</h3><h4 id="双人成行">双人成行</h4><figure><img src="https://s2.loli.net/2022/07/20/dxheYjZ9R7vU5iw.png" alt="双人成行.png"><figcaption aria-hidden="true">双人成行.png</figcaption></figure><p>游玩：本人也是正经的玩过全流程的，和 Z 同学一点点摸索着玩，但是由于origin平台实在是连接困难，加上网络不好，在线游玩属实是体验不好，总是会掉线，玩着玩着就开始卡顿，掉线，每次重新连接大概五分钟，挺折磨人的耐心。</p><p>另外关于游玩方面，Z同学和我都不是高玩，玩起来其实也挺痛苦的，某些关卡对我们来说还挺需要操作和反应速度的，比如有的关卡自己需要在几秒内就跑到指定的位置，错了就要重来，但好在只要成功一次就能够过关，后面会自动保存。有些Boss 打起来也挺费时间的，例如有个黄蜂的Boss，那个需要科迪先喷糖浆，小梅后面才能够攻击，而 Boss还有一些小兵蜜蜂保护，这是一个需要跨过去的坎，游玩过程自己的鼠标都快费了，虽然是自动瞄准，但也是有判定范围的，最后鼠标都移动困难了。</p><p>关卡在设计方面非常有新意，是我玩过的游戏中不可多得的必玩游戏，可惜需要有一个朋友，配置要求极高。前期的关卡设计挺多，也有剧情，不过都Q掉了，后面几大章节明显不如前几关，而且神庙逃亡太多了，颜色也过于鲜艳，属于虎头蛇尾了，但是质量还是高于大多数同类游戏。音乐方面不评价，没咋注意，游戏难度听说是动态的，在某一关待久了听说会降低难度，方便手残玩家，但耗时间，适合平时时间长放松可以试一下，没时间不适合玩，比如累死晚上十点回家，这个游戏建议别试着玩了。</p><h4 id="影之诗">影之诗</h4><figure><img src="https://s2.loli.net/2022/07/20/jKwHk72dgOFYVmi.jpg" alt="影之诗.jpg"><figcaption aria-hidden="true">影之诗.jpg</figcaption></figure><p>游玩：算是老玩家了，在起焉之光版本入坑，大概一两周后碧蓝骑士版本（在国服，当时国际服应该早就碧蓝骑士版本了），中间退坑过一年的时间，大概就是疫情那段时间，去玩昆特牌了，没碰影之诗，拿过三个版本GM，优点是无聊，缺点也是无聊。国服的福利极好，随便肝一下每日任务就行，基本上每个版本的所有卡都能拿到，还会有剩余的资源。但是为什么退坑，就是太无聊了，稍微娱乐一点点的卡组就不能玩，而且渐渐的很多东西都脱离实际了，很多职业特色都没有了，像暗夜伯爵原本非常好的一张鬼妈（8费用，4 攻击，4生命，虹卡，荣耀再临版本），原来效果是从卡堆里抽到手上时消失，直接无条件复仇，俗称点灯，后来削过后就没意思了，主战者生命上限直接少了一半，对手稍微打一下就输了，而且国服版本特别难玩，稍微有一点不顺就会输，任务有时候打三四个小时都做不好，而且后面设计的卡越来越膨胀，膨胀就算了，还膨胀的特别快，官方削弱卡牌一削弱就不能用，会不会削弱啊，卡牌不会设计就算了，削弱也是一刀切。我最关心的其实是双人模式，可惜一直没出，出不出都不会去玩了，毕竟是二次元游戏，我反而重视游戏性，玩steam 挺好。</p><h4 id="洛克王国">洛克王国</h4><figure><img src="https://s2.loli.net/2022/07/20/lRL6wrIn75jkmyX.png" alt="洛克王国.png"><figcaption aria-hidden="true">洛克王国.png</figcaption></figure><p>游玩：也算是个老玩家，基本前期游玩的样子都知道，疫情的时候也是玩到了高级训练师，系别排位也是圣魔导师。前期洛克王国感觉更像是经营类的游戏，战斗没有任何技巧可言，就是想办法控制对面（催眠粉，以及后面的瓦斯叮当的生命火焰，或者雪影娃娃的冰晶结界和冰龙王的嗜血寒冰冰冻），找一个C位进行强化进行推队，简单得很，看运气。但是现在的战斗很有趣，基本都是会玩的能够碾压不会玩的，已经不是以前的过家家了，从前期的收集、逛街重点倾向于战斗了，我玩过，很有趣，但是也无聊，每场战斗少则二十分钟，多则上小时，每天十把，累死人，而且网络极度不稳定，极度极度不稳定，退坑原因一方面就是这个，战斗我觉得比王者啥的好多了，操作运营一点不输王者那些游戏，另一方面是flash游戏，已经淘汰的技术，网页大多数不支持且有广告。另外前期洛克王国剧情极好，后期就是现在的剧情完全就是白开水，只会玩梗，一点感情没有，看其他大佬分析，游戏封面也敷衍起来了，以前是剧情为主，每周任务也很好，至少挺人性化，现在随便放几个花盆宠，而且每周任务特别恶心人，费时费力，不用辅助手刷我是时间多了没事干啊，游戏已经到了末期了，另外论坛也是很让人头疼的地方。最后提一嘴，氪金吗？也挺氪金的，不过比不上王者一套皮肤的钱，不氪金可以玩，也是大佬，要肝一肝的。最近洛克王国手游也被大众关注，我不看好，不为什么，就是不看好，洛克王国页游都不行，手游也大概率不行，都是刷刷刷的，剧情和所寄托的文化没有，算不上好游戏。原神也火出去了啊？我不玩原神，但那个是手游和PC 端互通啊，手机方便，但其性能现在远远弱于电脑。</p><h4 id="觅长生">觅长生</h4><figure><img src="https://s2.loli.net/2022/07/20/fHcmsZepbixJzMv.png" alt="觅长生.png"><figcaption aria-hidden="true">觅长生.png</figcaption></figure><p>游玩：依然是 steam 游戏，我很早就在 B 站上看见了，当时还没 ea版本呢，可惜当时不看好，卡牌？？金木水火土的牌能做出来什么战斗呢，就没管，最近改成灵气球了，无所谓，玩多了都可以。后来鬼谷八荒火了，当时也不知道怎么回事，对修仙游戏极度渴望，找到了觅长生，当时鬼谷八荒还没出事，就是bug多，更新慢，饼很多。当时看了飞羽仙狂的游玩，会了打劫流，渐渐熟悉了，制作组诚意很足，每周都更新，还有了B 站的wiki，体系完善，就等制作组继续更新了，不过好慢啊，都好多年了，还在 ea阶段，游戏最近的创意工坊也出来了，玩家实力大大增加，难度降低很多，不过觅长生我更喜欢看看剧情了，飞不飞升要等正式版了（虽然我也化神了），喜欢修仙游戏的可以尝试，至少没有坑，也没有饼，缺点就像steam里面说的，可以玩的修仙小说，看剧情和装大佬才是王道啦。另外有一款了不起的修仙模拟器（玩法是修仙门派模拟器）也很好，但学习成本高。有额外资金且对修仙感兴趣可以尝试这两款游戏（觅长生和了不起的休闲模拟器，鬼谷八荒有大坑，别去）。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>各种功能测试</title>
    <link href="/2024/07/28/%E5%90%84%E7%A7%8D%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    <url>/2024/07/28/%E5%90%84%E7%A7%8D%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h4 id="latex-公式">LaTex 公式</h4><p><span class="math display">\[J_r^{-1}(e_{ij})\approx I +\frac { 1 } { 2 }\begin{bmatrix}\phi_e^ { \wedge } &amp; \rho_e^ { \wedge } \\0 &amp; \phi_e^{\wedge} \\\end{bmatrix}\]</span></p><p><span class="math display">\[E=mc^2\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>github部署个人博客</title>
    <link href="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h4 id="参考链接">参考链接</h4><p><a href="https://blog.csdn.net/clearloe/article/details/139879493">Hexo博客搭建并部署到 GitHub Pages(2024 最新详细版)</a></p><h4 id="安装-vscode">安装 VsCode</h4><p><a href="https://blog.csdn.net/msdcp/article/details/127033151">VSCode安装配置使用教程（最新版超详细保姆级含插件）一文就够了</a></p><p><a href="https://code.visualstudio.com/Download">VsCode下载链接</a></p><ul><li>如果无法下载，可以选择在电脑的应用商城里面下载，我记得是要挂梯子才能官网下载</li><li>安装就是正常安装，建议别 C 盘</li></ul><h4 id="安装-git">安装 Git</h4><p><a href="https://blog.csdn.net/qq_45730223/article/details/131693287">Git安装详解（写吐了，看完不后悔）</a></p><ul><li>很常用的软件，程序员必装，按照教程装就好了，不过有时候验证有问题，也要梯子</li></ul><h4 id="安装-nodejs">安装 nodejs</h4><p><a href="https://blog.csdn.net/weixin_44893902/article/details/121788104">node.js安装及环境配置超详细教程【Windows 系统安装包方式】</a></p><blockquote><p>搬运上述链接评论 Node.js Express 安装报错总结 express 4.x 版本之前全局安装 express 命令是 npm install express -g express 4.x 版本之后全局安装 express 命令是 npm install -g express-generator</p></blockquote><h4 id="安装-hexo">安装 hexo</h4><p><a href="https://www.cnblogs.com/zhujingxiu/articles/7462025.html" title="发布于 2017-09-01 10:34">Win10 任意目录下默认快速以管理员身份运行CMD</a> <a href="https://blog.csdn.net/clearloe/article/details/139879493">安装hexo 参考博客链接</a> <a href="https://blog.csdn.net/qq_42786011/article/details/123895927">运行npm install 不动时</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">//  配置nmp代理来提高速度，如设置淘宝镜像<br>npm config set registry  https://registry.npmmirror.com/<br><br>// 查看配置是否成功<br>npm config get registry<br><br>// 成功后重新npm install安装<br>// npm install 。。。<br></code></pre></td></tr></table></figure><h4 id="遇上的错误解决方法供参考">遇上的错误，解决方法供参考</h4><h5 id="fatal-detected-dubious-ownership-in-repository">fatal: detecteddubious ownership in repository</h5><p><a href="https://stackoverflow.com/questions/73408170/git-fatal-detected-dubious-ownership">参考博客</a></p><img src="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/202407281829173.png" class width="202407281829173"><ul><li>解决方法</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git config --global --add safe.directory *<br></code></pre></td></tr></table></figure><h5 id="安装-pandoc运行-pandoc-有误说找不到命令">安装 Pandoc，运行Pandoc 有误，说找不到命令</h5><blockquote><p>主要问题是我的 360 会自动删除 pandoc.exe,我在 360软件管家中的隔离区恢复了就行</p></blockquote><p><a href="https://github.com/jgm/pandoc/blob/master/INSTALL.md">pandoc安装地址</a></p><p>正常安装是有下面的四个文件，如果少了 exe 需要按照上面的进行恢复</p><img src="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/202407281832443.png" class width="202407281832443"><h5 id="hexo-使用-markdown-插入图片无法显示解决方法">Hexo 使用 markdown插入图片无法显示解决方法</h5><p><a href="https://www.jianshu.com/p/04814a816caf">参考博客</a></p><ul><li><p>安装插件</p><ul><li><p>先进入自己 blog 的目录 <img src="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/image-20240728203701626.png" class title="image-20240728203701626"></p></li><li><p>用命令安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install https://github.com/7ym0n/hexo-asset-image --save<br></code></pre></td></tr></table></figure></li></ul></li><li><p>修改 hexo 根目录下的_config.yml 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">post_asset_folder: <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># URL</span><br><span class="hljs-comment">## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;</span><br>url: http://yourname.github.io<br></code></pre></td></tr></table></figure></li><li><p>插入图片</p><ul><li>我用的是 typora，直接设置如下，然后复制图片，直接到文件中即可</li></ul><img src="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/image-20240728204034558.png" class title="image-20240728204034558"><img src="/2024/07/28/github%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/image-20240728204107912.png" class title="image-20240728204107912"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">![xx](xx/xx.png)<br></code></pre></td></tr></table></figure></li></ul><h4 id="其他可能用得上的网址">其他可能用得上的网址</h4><p><a href="https://hexo.io/zh-cn/docs">Hexo 官方中文文档</a> <a href="https://hexo.fluid-dev.com/docs/">Hexo Fluid 用户手册</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/07/28/hello-world/"/>
    <url>/2024/07/28/hello-world/</url>
    
    <content type="html"><![CDATA[<h4 id="博客信息">博客信息</h4><p>域名：wing2791.cn</p><ul><li>已经弃用，但是域名尚未过期（20240728 注释）</li></ul><p>博客创建时间：2021 年 11 月</p><ul><li>最初的博客服务器快过期了，也不想续费了，转移到 github上了，后续会慢慢把笔记转移到这里</li></ul><p>网站负责人：wing2791</p><p>联系方式：</p><p>邮箱：wing2791@163.com B 站：<a href="https://space.bilibili.com/384847565">wing2791</a></p><h5 id="博客主要内容">博客主要内容</h5><p>2022 年：打算添加自己平时学习的笔记，包括但不限于代码-杂谈。</p><p>2023 年：</p><ul><li>笔记（包含编程学习等）</li><li>杂谈（某个游戏或者动画自己的看法，小破网站应该没人知道吧，我写的保守点吧）</li></ul><p>2024 年:</p><ul><li>笔记（包含编程学习，论文阅读）</li><li>杂谈（某个游戏或者动画自己的看法，小破网站应该没人知道吧，我写的保守点吧）</li><li>成长</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
